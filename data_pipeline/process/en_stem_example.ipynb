{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.13 64-bit ('ja_nlp_python37': conda)",
   "metadata": {
    "interpreter": {
     "hash": "69b51278c896c302cd20549358ba584416498d33e69d1966a44dcb7763188691"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "环境配置见environment.txt\n",
    "\n",
    "如果第一个cell运行之后，报了ModuleNotFoundError，检查右上角的ipykernel是否和左下角环境配置中的环境一致。e.g.左下角环境是ja_nlp_python37，则右上角的的ipykernel需要是该环境下的ipykernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### english text preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\zhouy217\\OneDrive -  \\Documents\\data_pipeline\\notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhouy217\\AppData\\Local\\Temp\\1\\jieba.cache\n",
      "Loading model cost 1.569 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "[nltk_data] Error loading wordnet: [WinError 10060] A connection\n",
      "[nltk_data]     attempt failed because the connected party did not\n",
      "[nltk_data]     properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zhouy217\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'c:\\\\Users\\\\zhouy217\\\\OneDrive -  \\\\Documents\\\\data_pipeline\\\\app')\n",
    " \n",
    "import utils as U\n",
    "import text_mining_preprocess_0712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            sentence\n",
       "0  I enjoyd the event which took place yesteday &...\n",
       "2  The link to the show is http://t.co/4ftYom0i I...\n",
       "3  Best of all, NLTK is a free, open source, comm...\n",
       "4  NLTK has been called \"a wonderful tool for tea...\n",
       "5                           Please switch to 3D View"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I enjoyd the event which took place yesteday &amp;...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The link to the show is http://t.co/4ftYom0i I...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Best of all, NLTK is a free, open source, comm...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NLTK has been called \"a wonderful tool for tea...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Please switch to 3D View</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from text_mining_preprocess_0712 import read_file_as_dataframe\n",
    "en_df=read_file_as_dataframe(\"c:\\\\Users\\\\zhouy217\\\\OneDrive -  \\\\Documents\\\\data_pipeline\\\\data\\\\en_test.xlsx\",col_list=[0],name_list=['sentence'],file_header=None,IFna='no')\n",
    "en_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The link to the show is httptcoftYomi Its awesome youll luv it HadFun Enjoyed BFN GN'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "def is_english(uchar):\n",
    "    if uchar==u'\\u0020' or uchar >= u'\\u0041' and uchar <= u'\\u007A':  # 判断一个uchar是否是汉字\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def allcontents(contents):\n",
    "    content = ''\n",
    "    for i in contents:\n",
    "        # print(i)\n",
    "        if is_english(i):\n",
    "            content = content+i\n",
    "    return content\n",
    "cc=allcontents(en_SentenceList[1])\n",
    "# en_SentenceList[1]\n",
    "cc\n",
    "# is_english('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'4321 The link to the show is   Its awesome youll luv it HadFun Enjoyed BFN GN'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "import html\n",
    "import re\n",
    "import string\n",
    "def rm_nontext(text):\n",
    "    '''\n",
    "    remove url and other punctuations\n",
    "    '''\n",
    "    text_rmurl=html.unescape(text)\n",
    "    text = re.sub(r'https?:\\/\\/.\\S+', \"\", text_rmurl)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n",
    "    text = \"\".join([i for i in text if i not in string.punctuation])#rm punctuation\n",
    "    return text\n",
    "# dd=rm_nontext(en_SentenceList[1])\n",
    "dd=rm_nontext('4321 The link to the show is www.baidu.com Its awesome youll luv it HadFun Enjoyed BFN GN')\n",
    "# dd=rm_nontext(en_SentenceList[4])\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I enjoyd the event which took place yesteday & I lovdddd itttt ! ',\n",
       " \"The link to the show is http://t.co/4ftYom0i It's awesome you'll luv it #HadFun #Enjoyed BFN GN\",\n",
       " 'Best of all, NLTK is a free, open source, community-driven project.',\n",
       " 'NLTK has been called \"a wonderful tool for teaching, and working in, computational linguistics using Python,\" and \"an amazing library to play with natural language.\"',\n",
       " 'Please switch to 3D View']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from text_mining_preprocess_0712 import TextMiningPreprocess\n",
    "en_text_mining=TextMiningPreprocess(en_df)\n",
    "en_SentenceList=en_text_mining.create_SentenceList()\n",
    "en_SentenceList[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['enjoyed', 'event', 'took', 'place', 'yesterday', 'loved', 'itt'],\n",
       " ['link', 'show', 'awesome', 'luv', 'fun', 'enjoyed', 'bin', 'gn'],\n",
       " ['best', 'lt', 'free', 'open', 'source', 'communitydriven', 'project']]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from text_mining_preprocess_0712 import TextMiningPreprocess\n",
    "en_WordList_in_SentenceList=en_text_mining.english_text_cleaning()\n",
    "en_WordList_in_SentenceList[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    " stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['enjoyed', 'event', 'took', 'place', 'yesterday', 'loved', 'itt'],\n",
       " ['link', 'show', 'awesome', 'luv', 'fun', 'enjoyed', 'bin', 'gn'],\n",
       " ['best', 'lt', 'free', 'open', 'source', 'communitydriven', 'project'],\n",
       " ['lt',\n",
       "  'called',\n",
       "  'wonderful',\n",
       "  'tool',\n",
       "  'teaching',\n",
       "  'working',\n",
       "  'computational',\n",
       "  'linguistics',\n",
       "  'using',\n",
       "  'python',\n",
       "  'amazing',\n",
       "  'library',\n",
       "  'play',\n",
       "  'natural',\n",
       "  'language'],\n",
       " ['please', 'switch', '3', 'view'],\n",
       " ['open', 'icon', 'please'],\n",
       " ['want', 'activate', 'ac'],\n",
       " ['want', 'turn', 'air', 'conditioner'],\n",
       " ['switch', 'ac', 'please'],\n",
       " ['help', 'open', 'ac'],\n",
       " ['ac', 'please'],\n",
       " ['wanna', 'activate', 'ac'],\n",
       " ['please', 'activate', 'icon'],\n",
       " ['want', 'start', 'icon', 'please'],\n",
       " ['help', 'switch', 'ac'],\n",
       " ['want', 'switch', 'icon', 'please'],\n",
       " ['window', 'raise'],\n",
       " ['close', 'car', 'sunshine'],\n",
       " ['close', 'car', 'sunshine', 'please'],\n",
       " ['turn', 'car', 'sunshine'],\n",
       " ['lower', 'ac', 'temperature'],\n",
       " ['set', 'temperature', 'ac'],\n",
       " ['im', 'bit', 'hot'],\n",
       " ['help', 'decrease', 'temperature'],\n",
       " ['little', 'hot'],\n",
       " ['navigate', 'bristol', 'blue', 'glass', 'company'],\n",
       " ['go', 'legal', 'wills', 'direct'],\n",
       " ['go', 'utility', 'arena', 'birmingham'],\n",
       " ['go', 'no1', 'harbourside'],\n",
       " ['wanna', 'songs', 'favourites'],\n",
       " ['play', 'apocalypse', 'paper', 'randall'],\n",
       " ['play', 'give', 'love', 'nile', 'rodgers', 'please'],\n",
       " ['hector', 'ferreira', 'treat', 'better', 'please']]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "import itertools\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords\n",
    "def english_text_cleaning(df,IFnum = 'no', IFstem='no'):\n",
    "    '''\n",
    "    return a 2d list\n",
    "    e.g.\n",
    "    'I enjoyd the event which took place yesteday & I lovdddd itttt ! ',\n",
    "    \"The link to the show is http://t.co/4ftYom0i It's awesome you'll luv it #HadFun #Enjoyed BFN GN\",\n",
    "    'Best of all, NLTK is a free, open source, community-driven project.'\n",
    "    output : \n",
    "    [['enjoyed', 'event', 'took', 'place', 'yesterday', 'loved', 'itt'],\n",
    "    ['link', 'show', 'awesome', 'luv', 'fun', 'enjoyed', 'bin', 'gn'],\n",
    "    ['best', 'lt', 'free', 'open', 'source', 'communitydriven', 'project']]\n",
    "    '''\n",
    "    def rm_nontext(text):\n",
    "        '''\n",
    "        remove url and other punctuations, remain numbers\n",
    "        '''\n",
    "        text_rmurl=html.unescape(text)\n",
    "        text = re.sub(r'https?:\\/\\/.\\S+', \"\", text_rmurl)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        text = re.sub(r'^RT[\\s]+', '', text)\n",
    "        text = \"\".join([i for i in text if i not in string.punctuation])#rm punctuation\n",
    "        return text\n",
    "    \n",
    "    def is_english(uchar):\n",
    "        print(uchar)\n",
    "        if uchar==u'\\u0020' or uchar >= u'\\u0041' and uchar <= u'\\u007A':  \n",
    "            # 判断一个uchar是否是英文字母、空格(u+0020)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def allcontents(text):\n",
    "        '''\n",
    "        remove all non-english text, including numbers and all punctuations\n",
    "        '''\n",
    "        content = ''\n",
    "        for i in text:\n",
    "            # print(i)\n",
    "            if is_english(i):\n",
    "                content = content+i\n",
    "        return content\n",
    "\n",
    "    def conv_text(text):\n",
    "        '''\n",
    "        Restore abbreviations to full names and lowercase form\n",
    "        '''\n",
    "        Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n",
    "                \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "        for key,value in Apos_dict.items():\n",
    "            if key in text:\n",
    "                text=text.replace(key,value)        \n",
    "            \n",
    "        text = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",text) if s])\n",
    "\n",
    "        text=text.lower()\n",
    "        ''' \n",
    "        slang lookup\n",
    "        file=open(\"./slang.txt\",\"r\")\n",
    "        slang=file.read()\n",
    "        slang=slang.split('\\n')\n",
    "        text_tokens=text.split()\n",
    "        slang_word=[]\n",
    "        meaning=[]\n",
    "\n",
    "        for line in slang:\n",
    "            temp=line.split(\"=\")\n",
    "            slang_word.append(temp[0])\n",
    "            meaning.append(temp[-1])\n",
    "\n",
    "        for i,word in enumerate(text_tokens):\n",
    "            if word in slang_word:\n",
    "                idx=slang_word.index(word)\n",
    "                text_tokens[i]=meaning[idx]\n",
    "                \n",
    "        text=\" \".join(text_tokens)\n",
    "        '''\n",
    "        #spell check\n",
    "        text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "        spell = Speller(lang='en')\n",
    "        #spell check\n",
    "        text=spell(text)\n",
    "        return text\n",
    "\n",
    "    def stem_lemma_word(text):\n",
    "        '''\n",
    "        stem words into their original form\n",
    "        e.g. working -> work\n",
    "        '''\n",
    "        text_tokens=text.split()#text_tokens is a word list for each sentence\n",
    "        porter = PorterStemmer()\n",
    "        stemmed_list = [porter.stem(word) for word in text_tokens]\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        lemmatize_words = np.vectorize(wordnet_lemmatizer.lemmatize)\n",
    "        lemmatized_list = [lemmatize_words(word) for word in stemmed_list]\n",
    "        return stemmed_list\n",
    "\n",
    "    def rm_stopwords(text):\n",
    "        '''remove stopwords'''\n",
    "        stopwords_eng = stopwords.words('english')\n",
    "        text_tokens=text.split()\n",
    "        text_list=[]\n",
    "        for word in text_tokens:\n",
    "            if word not in stopwords_eng:\n",
    "                text_list.append(word)\n",
    "        return text_list\n",
    "\n",
    "    if IFnum=='yes' and IFstem == 'yes':\n",
    "        funcs=[rm_nontext,conv_text,stem_lemma_word,rm_stopwords]\n",
    "    elif IFnum=='yes' and IFstem == 'no':\n",
    "        funcs=[rm_nontext,conv_text,rm_stopwords]\n",
    "    elif IFnum=='no' and IFstem == 'yes':\n",
    "        funcs=[allcontents,conv_text,stem_lemma_word,rm_stopwords]\n",
    "    elif IFnum=='no' and IFstem == 'no':\n",
    "        funcs=[allcontents,conv_text,rm_stopwords]\n",
    "    # funcs=[rm_nontext,conv_text,rm_stopwords]\n",
    "    def func_list(data):\n",
    "        for func in funcs:\n",
    "            data=func(data)\n",
    "        return data\n",
    "    en_2d_list=[]\n",
    "    for en_sen in en_SentenceList:\n",
    "        cleaned_en_word_list=func_list(en_sen)\n",
    "        en_2d_list.append(cleaned_en_word_list)\n",
    "    en_WordList_in_SentenceList=en_2d_list\n",
    "    return en_2d_list\n",
    "ee=english_text_cleaning(en_df,IFnum='yes')\n",
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def stem_word(text):\n",
    "    text_tokens=text.split()#text_tokens is a word list for each sentence\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_list = [porter.stem(word) for word in text_tokens]\n",
    "    return stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'call',\n",
       " '\"a',\n",
       " 'wonder',\n",
       " 'tool',\n",
       " 'for',\n",
       " 'teaching,',\n",
       " 'and',\n",
       " 'work',\n",
       " 'in,',\n",
       " 'comput',\n",
       " 'linguist',\n",
       " 'use',\n",
       " 'python,\"',\n",
       " 'and',\n",
       " '\"an',\n",
       " 'amaz',\n",
       " 'librari',\n",
       " 'to',\n",
       " 'play',\n",
       " 'with',\n",
       " 'natur',\n",
       " 'language.\"']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "stem_word(en_SentenceList[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'librari'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "stemmed_list1 = porter.stem(\"library\") \n",
    "stemmed_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_list = en_text_mining.flatten_list()\n",
    "# en_word_list\n",
    "en_concat_sentence_text = en_text_mining.concat_sentence()\n",
    "# en_concat_sentence_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
