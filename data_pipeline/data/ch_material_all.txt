本文概述了句子对齐过程以及如何使用可扩展的管道构建它们。我们还讨论了作为 Project Anuvaad（一个基于印度语的翻译系统）的一部分在执行大规模对齐方面所面临的挑战，以及如何使用 FAISS 对其进行优化以显着提高双语语料库的管理性能。我们是印度最大的公开平行语料库 samanantar 的最大贡献者之一。我们也将成为 ULCA（通用语言贡献 API）系统的早期贡献者。
双语句子对齐
双语句子对齐（或跨语言语义相似性）被定义为搜索两个可用的句子集合并在其中识别有意义匹配的句子对（如果有）。
首先，要详细了解整个过程，让我们从基础开始。考虑两个具有相似（但不完全匹配）句子的列表。
aligner 背后的想法是识别“我的名字是阿斯温”和“阿斯温，那是我的名字”这样的情况都是相似的。
尽管问题乍一看似乎很复杂，但有许多方法可以实现这一目标。可以使用 Fuzzywuzzy、Word2vec、TF-IDF 等基本方法来实现相同的目的。不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论。但是，这里的想法是要确定如果语言对是英语-英语，这样做相对简单。可以参考这篇文章来详细了解这里的各种方法。
下面的代码解释了一种在语言为 en-en 时查找匹配对的简单方法。
即使这种简单的方法在基本情况下有效，我们也不能指望它在复杂和模棱两可的情况下提供理想的输出。因此，我们需要切换到基于模型的方法。此外，由于我们的目标是对双语文本进行对齐，现在让我们从现在开始在这种情况下思考，因为同样的方法也可以应用于单语案例。
为了实现这一点，我们利用预训练的编码器-解码器模型将这些句子转换为向量表示，然后使用这些向量表示（称为嵌入）来计算句子之间的相似度。在进一步搜索中，我们入围了两个模型，LASER（来自 Facebook）和 LaBSE（来自 Google）。在本文中，Vishal Mahuli 解释了它们之间的比较。由于 LaBSE 为印度语言提供了更好的输出，因此它被用作 Anuvaad 项目 (https://anuvaad.org/) 的一部分。
基本实现
使用 LaBSE 为源语言和目标语言句子的所有句子生成向量嵌入。计算每个句子对之间的余弦距离，具有最大余弦相似度的句子对将被视为一对。
这是一种使用包装库 polyfuzz 的基本方法，它计算 BERT 嵌入并基于它进行匹配。输出如下所示。
没有为此图像提供替代文字

我们在没有 polyfuzz 的情况下手动尝试了相同的方法，并使用各种参数微调了方法和阈值，并以稍微不同的方式实现了相同的想法，可以在这里找到。
全部完成？
问题解决了，我们开始使用 LaBSE 在各种印度语言中获得有意义的配对。然而，系统性能受到了影响，处理时间随着输入数据呈指数增长。当输入句子数超过 100K 时，它甚至需要几天时间。
这导致了一种在不影响输出质量的情况下微调对齐的方法。
最初，尝试了基本方法，例如将模型保存到本地并加载并一次又一次地使用它，在 GPU 上运行嵌入计算等。
通过切换到 GPU，使用 LaBSE 进行矢量化的时间显示出显着差异，但是，余弦相似度计算和蛮力匹配方法仍然很耗时。文件末尾列出了注明的时间。总体时间减少到一半，但是，当数据集大小增加时，它仍然缓慢并且时间呈指数增长。
然后，我们搜索并尝试了多种方法来实现向量之间的有效相似性搜索，并在 KNN 和 FAISS 中落地。
K 最近邻
KNN 的工作原理是找出查询与数据中所有示例之间的距离，选择最接近查询的指定数量的示例 (K)，然后投票选出最频繁的标签（在分类的情况下）或平均标签 (在回归的情况下）。 KNN 也可用于高效搜索。
KNN 算法
1.加载数据
2. 将 K 初始化为您选择的邻居数
3.对于数据中的每个例子
从数据中计算查询示例与当前示例之间的距离。
将示例的距离和索引添加到有序集合中。
4. 按距离升序对有序的距离和索引集合进行排序。
5. 从已排序的集合中挑选前 K 个条目
6. 获取选中的K个条目的标签
7. 如果需要，执行回归/分类
在这里，我们不使用 KNN 进行任何预测，而仅用于相似性搜索。有多种计算距离的方法，根据我们要解决的问题，一种方法可能更可取。 KNN 的主要缺点是随着输入样本数量的增加，算法变得明显变慢。这就是 FAISS 发挥作用的地方。
法斯
FAISS（Facebook AI Similarity Search）是Facebook开发的一个用于高效相似度搜索和密集向量聚类的库。 FAISS 是用 C++ 编写的，带有 Python/numpy 的完整包装器。它包含在任意大小的向量集中搜索的算法，直到那些可能不适合 RAM 的向量。一些最有用的算法在 GPU 上实现以提高性能。
FAISS 是围绕一种索引类型构建的，该索引类型存储了一组向量，并提供了一个函数来通过 L2 和/或点积向量比较在其中进行搜索。一些索引类型是简单的基线，例如精确搜索。
FAISS 经过调整，与现有的库（如 Scikit Learn）相比，它极大地缩小了向量 k 最近邻的搜索空间，以便在向量之间进行更快的相似性搜索。这种技术被称为近似最近邻 (ANN) 搜索，并牺牲了一些精度来获得巨大的加速。
初步准备
我们已经对 FAISS 和其中的各种索引进行了几次实验。但是，以下是所有人都通用的一些准备步骤。初始化模型并基于它对嵌入进行编码。这些嵌入被传递给 KNN 函数，该函数根据可以重新创建的匹配数据集返回相似度分数和相应的列表索引。
没有为此图像提供替代文字
FAISS 指数
FAISS 是围绕 Index 对象构建的。它封装了一组数据库向量，并可选择对它们进行预处理以提高搜索效率。有许多类型的索引。
平面索引只是将向量编码为固定大小的代码，并将它们存储在 ntotal * code_size 字节的数组中。在搜索时，所有索引向量都被顺序解码并与查询向量进行比较。在平面索引 (IndexFlat) 中：向量存储时不进行压缩。
FAISS 中的另一类索引是基于分区的索引，它通过将索引划分为集群并将搜索限制在几个集群中来加速搜索。然而，这种方法并不准确，因为不能保证最近的邻居会在搜索的集群中。使用分区技术使搜索空间更小、更高效的索引示例是 IndexIVFFlat 索引。
搜索操作可以按照与早期索引相同的方式进行。但是，在 IVFFlat 索引中，我们定义了“nprobe”超参数，以将搜索限制为仅与查询向量最近的定义数量的集群。这也是如何将不同的索引组合成单个索引的示例。
下面给出了我们尝试过的索引的表格表示
实验一：人工神经网络
由于速度是与基本实现的主要权衡，我们最初的方法是尝试在执行速度方面提供最佳性能的方法。
以失去找到最近邻居的保证为代价来加速该过程的典型方法是采用分区技术，例如 k-means。相应的算法有时被称为细胞探测方法。
我们使用基于多探测的基于分区的方法。
特征空间被划分为 nlist 单元。
使用量化函数将数据库向量分配给这些单元之一（在 k-means 的情况下，分配给最接近查询的质心），并存储在由 nlist 倒排列表形成的倒排文件结构中。
查询时，选择一组nprobe倒排列表
将查询与分配给这些列表的每个数据库向量进行比较。
这样做，只有一部分数据库与查询进行比较：作为第一个近似值，这个部分是 nprobe/nlist，但是这个近似值通常被低估，因为倒排列表的长度不相等。当未选择给定查询的最近邻居的单元格时会出现失败情况。
构造函数将索引作为参数（量化器或粗量化器），用于对倒排列表进行赋值。在该索引中搜索查询，返回的向量 id(s) 是应该访问的倒排列表。
没有为此图像提供替代文字
带走
超快执行。
输出足够好，但对于并行语料库数据集的创建来说质量不佳。
无论速度如何，基本实现都能提供更好的结果。
实验 2：余弦相似度测量
由于基于余弦相似度的初始基本实现比基于 ANN 的方法提供了更好的质量输出，接下来的想法是尝试一下。在 FAISS 中，我们没有余弦相似度方法，但有计算向量之间的内积或点积的索引。然后我们可以利用余弦相似度只是归一化向量之间的点积这一事实。
下面的代码片段显示了如何实现这一点。
没有为此图像提供替代文字
带走
正如预期的那样，搜索比以前的方法慢一点。
产出质量有所提高，但仍达不到标准。
当阈值降低时，质量会显着降低。
基本实现仍然提供了更好的输出。