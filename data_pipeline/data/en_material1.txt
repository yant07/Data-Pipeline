The article gives an overview of the sentence alignment process and how to build them using a scalable pipeline. We also talk about the challenges faced as part of the Project Anuvaad (an Indic based translation system) in performing alignment at scale and how it was optimized using FAISS for considerable performance improvement in curating the bi-lingual corpus. We are one of the biggest contributors to India’s largest publicly available parallel corpus samanantar. We will also be the early contributors to the ULCA ( Universal Language Contribution API ) system.
Bilingual Sentence Alignment
Bilingual Sentence alignment (or cross-language semantic similarity) is defined as searching through two available collections of sentences and identifying meaningfully matching sentence pairs (if any) among them.
First, to get a detailed idea about the whole process, let’s start from the basics. Consider two lists, having similar (but not exactly matching) sentences.
The idea behind aligner is to identify cases like “My name is Aswin” and “Aswin, thats my name” are both similar.
Even Though the problem seems complex at the first glance, there are numerous methods available for achieving this. Basic methods like Fuzzywuzzy, Word2vec, TF-IDF, etc could be used to achieve the same. Extracting out quality pairs in complex inputs is a bit tricky though, which we will discuss later. However, the idea here is to establish the fact that it’s relatively simple to do it if the language pair is English-English. One can refer to this article to understand varied approaches in detail here.
The code below explains a simple approach to find matching pairs if language is en-en.
Even if such simple approaches work out in basic situations, we cannot expect it to provide ideal output in complex and ambiguous cases. Therefore, we need to switch to a model-based approach. Also since we aim to do the alignment for bilingual texts, now let’s think in that context from now on, since the same approach could be applied for the monolingual cases as well.
In order to achieve this, we make use of pre-trained encoder-decoder models that transform these sentences into vector representations, these vector representations (known as embeddings) are then used to calculate the similarity between the sentences. On further search, we shortlisted two models, LASER (from Facebook) and LaBSE (from Google). In this article, Vishal Mahuli explains the comparison between both of them. Since LaBSE gave considerably better outputs for Indic languages, it was used as part of the Anuvaad project (https://anuvaad.org/).
Basic Implementation
Generate vector embeddings for all the sentences for source and target languages sentences with LaBSE. Calculate the cosine distances between each of these sentence pairs and the couple that has the largest cosine similarity will be considered to be a pair.
