{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.13 64-bit ('ja_nlp_python37': conda)",
   "metadata": {
    "interpreter": {
     "hash": "69b51278c896c302cd20549358ba584416498d33e69d1966a44dcb7763188691"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 准备部分，包含读文件(read_text)，以及将文本切分为句子(split_into_en_sentences,split_into_ch_sentences)\n",
    "#### 这三个函数写在了./app/utils.py中，使用前import utils即可"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = ''.join(file.readlines())\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "def split_into_en_sentences_modified_1(text):\n",
    "    '''<stop>是人为添加的'标识符'，目的是为了最后统一用<stop>来split句子\n",
    "    参考https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences'''\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\"<stop>\")\n",
    "    '''修改的地方：将(\"\\n\",\" \")改为(\"\\n\",\"<stop>\")\n",
    "    为了使小标题或其他有换行但没有标点的地方，也可以顺利分开'''\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    while '' in sentences:\n",
    "        sentences.remove('')\n",
    "    '''\n",
    "    由于最开始将\\n替换为了<stop>，而<stop>最后会作为分隔符。如果没有remove这行的话，分隔符的地方会多出空的字符串\n",
    "    '''\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_ch_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\"<stop>\")\n",
    "    if \"”\" in text: text = text.replace(\"。”\",\"”。\")\n",
    "    if \"\\\"\" in text: text = text.replace(\"。\\\"\",\"\\\"。\")\n",
    "    if \"！\" in text: text = text.replace(\"！\\\"\",\"\\\"！\")\n",
    "    if \"？\" in text: text = text.replace(\"？\\\"\",\"\\\"？\")\n",
    "    text = text.replace(\"。\",\"。<stop>\")\n",
    "    text = text.replace(\"？\",\"？<stop>\")\n",
    "    text = text.replace(\"！\",\"！<stop>\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    while '' in sentences:\n",
    "        sentences.remove('')\n",
    "    '''\n",
    "    由于最开始将\\n替换为了<stop>，而<stop>最后会作为分隔符。如果没有remove这行的话，分隔符的地方会多出空的字符串\n",
    "    '''\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = read_text('c:\\\\Users\\\\zhouy217\\\\OneDrive - Pfizer\\\\Documents\\\\data_pipeline\\\\data\\\\text2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"I enjoyd the event which took place yesteday & I lovdddd itttt ! \\nThe link to the show is http://t.co/4ftYom0i. It's awesome you'll luv it #HadFun #Enjoyed BFN GN\\n\\nBest of all, NLTK is a free, open source, community-driven project.\\n\\nNLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”\""
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "en_text#如运行结果所示，保留了换行符\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I enjoyd the event which took place yesteday & I lovdddd itttt !',\n",
       " 'The link to the show is http://t.',\n",
       " 'co/4ftYom0i.',\n",
       " \"It's awesome you'll luv it #HadFun #Enjoyed BFN GN\",\n",
       " 'Best of all, NLTK is a free, open source, community-driven project.',\n",
       " 'NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language”.']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "en_result_list=split_into_en_sentences_modified_1(en_text)\n",
    "en_result_list"
   ]
  },
  {
   "source": [
    "### 主体用LaBSE做text alignment部分,match部分的代码写在了./app/text_alignment.py中的sentence_similarity_calculate函数里\n",
    "#### 下面展示了polyfuzz的使用例子，包括polyfuzz结合LaBSE并用一篇英文及中文译文作了试验、polyfuzz的匹配规则。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ANACONDA3\\envs\\ja_nlp_python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from polyfuzz import PolyFuzz\n",
    "from polyfuzz.models import Embeddings\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.embeddings import SentenceTransformerDocumentEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerDocumentEmbeddings('LaBSE')\n",
    "LaBSE = Embeddings(embeddings,min_similarity=0,model_id='LaBSE')\n",
    "model=PolyFuzz([LaBSE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The article gives an overview of the sentence alignment process and how to build them using a scalable pipeline.',\n",
       " 'We also talk about the challenges faced as part of the Project Anuvaad (an Indic based translation system) in performing alignment at scale and how it was optimized using FAISS for considerable performance improvement in curating the bi-lingual corpus.',\n",
       " 'We are one of the biggest contributors to India’s largest publicly available parallel corpus samanantar.',\n",
       " 'We will also be the early contributors to the ULCA ( Universal Language Contribution API ) system.',\n",
       " 'Bilingual Sentence Alignment',\n",
       " 'Bilingual Sentence alignment (or cross-language semantic similarity) is defined as searching through two available collections of sentences and identifying meaningfully matching sentence pairs (if any) among them.',\n",
       " 'First, to get a detailed idea about the whole process, let’s start from the basics.',\n",
       " 'Consider two lists, having similar (but not exactly matching) sentences.',\n",
       " 'The idea behind aligner is to identify cases like “My name is Aswin” and “Aswin, thats my name” are both similar.',\n",
       " 'Even Though the problem seems complex at the first glance, there are numerous methods available for achieving this.',\n",
       " 'Basic methods like Fuzzywuzzy, Word2vec, TF-IDF, etc could be used to achieve the same.',\n",
       " 'Extracting out quality pairs in complex inputs is a bit tricky though, which we will discuss later.',\n",
       " 'However, the idea here is to establish the fact that it’s relatively simple to do it if the language pair is English-English.',\n",
       " 'One can refer to this article to understand varied approaches in detail here.',\n",
       " 'The code below explains a simple approach to find matching pairs if language is en-en.',\n",
       " 'Even if such simple approaches work out in basic situations, we cannot expect it to provide ideal output in complex and ambiguous cases.',\n",
       " 'Therefore, we need to switch to a model-based approach.',\n",
       " 'Also since we aim to do the alignment for bilingual texts, now let’s think in that context from now on, since the same approach could be applied for the monolingual cases as well.',\n",
       " 'In order to achieve this, we make use of pre-trained encoder-decoder models that transform these sentences into vector representations, these vector representations (known as embeddings) are then used to calculate the similarity between the sentences.',\n",
       " 'On further search, we shortlisted two models, LASER (from Facebook) and LaBSE (from Google).',\n",
       " 'In this article, Vishal Mahuli explains the comparison between both of them.',\n",
       " 'Since LaBSE gave considerably better outputs for Indic languages, it was used as part of the Anuvaad project (https://anuvaad.org/).',\n",
       " 'Basic Implementation',\n",
       " 'Generate vector embeddings for all the sentences for source and target languages sentences with LaBSE.',\n",
       " 'Calculate the cosine distances between each of these sentence pairs and the couple that has the largest cosine similarity will be considered to be a pair.']"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "en_sentences = read_text('c:\\\\Users\\\\zhouy217\\\\OneDrive - Pfizer\\\\Documents\\\\data_pipeline\\\\data\\\\en_material1.txt')\n",
    "en_result_list=split_into_en_sentences_modified_1(en_sentences)\n",
    "en_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "len(en_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['本文概述了句子对齐过程以及如何使用可扩展的管道构建它们。',\n",
       " '我们还讨论了作为 Project Anuvaad（一个基于印度语的翻译系统）的一部分在执行大规模对齐方面所面临的挑战，以及如何使用 FAISS 对其进行优化以显着提高双语语料库的管理性能。',\n",
       " '我们是印度最大的公开平行语料库 samanantar 的最大贡献者之一。',\n",
       " '我们也将成为 ULCA（通用语言贡献 API）系统的早期贡献者。',\n",
       " '双语句子对齐',\n",
       " '双语句子对齐（或跨语言语义相似性）被定义为搜索两个可用的句子集合并在其中识别有意义匹配的句子对（如果有）。',\n",
       " '首先，要详细了解整个过程，让我们从基础开始。',\n",
       " '考虑两个具有相似（但不完全匹配）句子的列表。',\n",
       " 'aligner 背后的想法是识别“我的名字是阿斯温”和“阿斯温，那是我的名字”这样的情况都是相似的。',\n",
       " '尽管问题乍一看似乎很复杂，但有许多方法可以实现这一目标。',\n",
       " '可以使用 Fuzzywuzzy、Word2vec、TF-IDF 等基本方法来实现相同的目的。',\n",
       " '不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论。',\n",
       " '但是，这里的想法是要确定如果语言对是英语-英语，这样做相对简单。',\n",
       " '可以参考这篇文章来详细了解这里的各种方法。',\n",
       " '下面的代码解释了一种在语言为 en-en 时查找匹配对的简单方法。',\n",
       " '即使这种简单的方法在基本情况下有效，我们也不能指望它在复杂和模棱两可的情况下提供理想的输出。',\n",
       " '因此，我们需要切换到基于模型的方法。',\n",
       " '此外，由于我们的目标是对双语文本进行对齐，现在让我们从现在开始在这种情况下思考，因为同样的方法也可以应用于单语案例。',\n",
       " '为了实现这一点，我们利用预训练的编码器-解码器模型将这些句子转换为向量表示，然后使用这些向量表示（称为嵌入）来计算句子之间的相似度。',\n",
       " '在进一步搜索中，我们入围了两个模型，LASER（来自 Facebook）和 LaBSE（来自 Google）。',\n",
       " '在本文中，Vishal Mahuli 解释了它们之间的比较。',\n",
       " '由于 LaBSE 为印度语言提供了更好的输出，因此它被用作 Anuvaad 项目 (https://anuvaad.org/) 的一部分。',\n",
       " '基本实现',\n",
       " '使用 LaBSE 为源语言和目标语言句子的所有句子生成向量嵌入。',\n",
       " '计算每个句子对之间的余弦距离，具有最大余弦相似度的句子对将被视为一对。']"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "ch_sentences = read_text('c:\\\\Users\\\\zhouy217\\\\OneDrive - Pfizer\\\\Documents\\\\data_pipeline\\\\data\\\\ch_material1.txt')\n",
    "ch_result_list=split_into_ch_sentences(ch_sentences)\n",
    "ch_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "len(ch_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 From  \\\n",
       "0   The article gives an overview of the sentence ...   \n",
       "1   We also talk about the challenges faced as par...   \n",
       "2   We are one of the biggest contributors to Indi...   \n",
       "3   We will also be the early contributors to the ...   \n",
       "4                        Bilingual Sentence Alignment   \n",
       "5   Bilingual Sentence alignment (or cross-languag...   \n",
       "6   First, to get a detailed idea about the whole ...   \n",
       "7   Consider two lists, having similar (but not ex...   \n",
       "8   The idea behind aligner is to identify cases l...   \n",
       "9   Even Though the problem seems complex at the f...   \n",
       "10  Basic methods like Fuzzywuzzy, Word2vec, TF-ID...   \n",
       "11  Extracting out quality pairs in complex inputs...   \n",
       "12  However, the idea here is to establish the fac...   \n",
       "13  One can refer to this article to understand va...   \n",
       "14  The code below explains a simple approach to f...   \n",
       "15  Even if such simple approaches work out in bas...   \n",
       "16  Therefore, we need to switch to a model-based ...   \n",
       "17  Also since we aim to do the alignment for bili...   \n",
       "18  In order to achieve this, we make use of pre-t...   \n",
       "19  On further search, we shortlisted two models, ...   \n",
       "20  In this article, Vishal Mahuli explains the co...   \n",
       "21  Since LaBSE gave considerably better outputs f...   \n",
       "22                               Basic Implementation   \n",
       "23  Generate vector embeddings for all the sentenc...   \n",
       "24  Calculate the cosine distances between each of...   \n",
       "\n",
       "                                                   To  Similarity  \n",
       "0                        本文概述了句子对齐过程以及如何使用可扩展的管道构建它们。       0.867  \n",
       "1   我们还讨论了作为 Project Anuvaad（一个基于印度语的翻译系统）的一部分在执行大...       0.923  \n",
       "2                我们是印度最大的公开平行语料库 samanantar 的最大贡献者之一。       0.840  \n",
       "3                    我们也将成为 ULCA（通用语言贡献 API）系统的早期贡献者。       0.959  \n",
       "4                                              双语句子对齐       0.806  \n",
       "5   双语句子对齐（或跨语言语义相似性）被定义为搜索两个可用的句子集合并在其中识别有意义匹配的句子...       0.913  \n",
       "6                              首先，要详细了解整个过程，让我们从基础开始。       0.880  \n",
       "7                              考虑两个具有相似（但不完全匹配）句子的列表。       0.918  \n",
       "8   aligner 背后的想法是识别“我的名字是阿斯温”和“阿斯温，那是我的名字”这样的情况都是...       0.889  \n",
       "9                        尽管问题乍一看似乎很复杂，但有许多方法可以实现这一目标。       0.910  \n",
       "10     可以使用 Fuzzywuzzy、Word2vec、TF-IDF 等基本方法来实现相同的目的。       0.937  \n",
       "11                       不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论。       0.832  \n",
       "12                   但是，这里的想法是要确定如果语言对是英语-英语，这样做相对简单。       0.898  \n",
       "13                              可以参考这篇文章来详细了解这里的各种方法。       0.793  \n",
       "14                  下面的代码解释了一种在语言为 en-en 时查找匹配对的简单方法。       0.876  \n",
       "15     即使这种简单的方法在基本情况下有效，我们也不能指望它在复杂和模棱两可的情况下提供理想的输出。       0.894  \n",
       "16                                 因此，我们需要切换到基于模型的方法。       0.931  \n",
       "17  此外，由于我们的目标是对双语文本进行对齐，现在让我们从现在开始在这种情况下思考，因为同样的方...       0.916  \n",
       "18  为了实现这一点，我们利用预训练的编码器-解码器模型将这些句子转换为向量表示，然后使用这些向量...       0.905  \n",
       "19  在进一步搜索中，我们入围了两个模型，LASER（来自 Facebook）和 LaBSE（来自...       0.973  \n",
       "20                     在本文中，Vishal Mahuli 解释了它们之间的比较。       0.927  \n",
       "21  由于 LaBSE 为印度语言提供了更好的输出，因此它被用作 Anuvaad 项目 (http...       0.928  \n",
       "22                                               基本实现       0.878  \n",
       "23                   使用 LaBSE 为源语言和目标语言句子的所有句子生成向量嵌入。       0.917  \n",
       "24                计算每个句子对之间的余弦距离，具有最大余弦相似度的句子对将被视为一对。       0.817  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>From</th>\n      <th>To</th>\n      <th>Similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The article gives an overview of the sentence ...</td>\n      <td>本文概述了句子对齐过程以及如何使用可扩展的管道构建它们。</td>\n      <td>0.867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>We also talk about the challenges faced as par...</td>\n      <td>我们还讨论了作为 Project Anuvaad（一个基于印度语的翻译系统）的一部分在执行大...</td>\n      <td>0.923</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>We are one of the biggest contributors to Indi...</td>\n      <td>我们是印度最大的公开平行语料库 samanantar 的最大贡献者之一。</td>\n      <td>0.840</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>We will also be the early contributors to the ...</td>\n      <td>我们也将成为 ULCA（通用语言贡献 API）系统的早期贡献者。</td>\n      <td>0.959</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bilingual Sentence Alignment</td>\n      <td>双语句子对齐</td>\n      <td>0.806</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Bilingual Sentence alignment (or cross-languag...</td>\n      <td>双语句子对齐（或跨语言语义相似性）被定义为搜索两个可用的句子集合并在其中识别有意义匹配的句子...</td>\n      <td>0.913</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>First, to get a detailed idea about the whole ...</td>\n      <td>首先，要详细了解整个过程，让我们从基础开始。</td>\n      <td>0.880</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Consider two lists, having similar (but not ex...</td>\n      <td>考虑两个具有相似（但不完全匹配）句子的列表。</td>\n      <td>0.918</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The idea behind aligner is to identify cases l...</td>\n      <td>aligner 背后的想法是识别“我的名字是阿斯温”和“阿斯温，那是我的名字”这样的情况都是...</td>\n      <td>0.889</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Even Though the problem seems complex at the f...</td>\n      <td>尽管问题乍一看似乎很复杂，但有许多方法可以实现这一目标。</td>\n      <td>0.910</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Basic methods like Fuzzywuzzy, Word2vec, TF-ID...</td>\n      <td>可以使用 Fuzzywuzzy、Word2vec、TF-IDF 等基本方法来实现相同的目的。</td>\n      <td>0.937</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Extracting out quality pairs in complex inputs...</td>\n      <td>不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论。</td>\n      <td>0.832</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>However, the idea here is to establish the fac...</td>\n      <td>但是，这里的想法是要确定如果语言对是英语-英语，这样做相对简单。</td>\n      <td>0.898</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>One can refer to this article to understand va...</td>\n      <td>可以参考这篇文章来详细了解这里的各种方法。</td>\n      <td>0.793</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>The code below explains a simple approach to f...</td>\n      <td>下面的代码解释了一种在语言为 en-en 时查找匹配对的简单方法。</td>\n      <td>0.876</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Even if such simple approaches work out in bas...</td>\n      <td>即使这种简单的方法在基本情况下有效，我们也不能指望它在复杂和模棱两可的情况下提供理想的输出。</td>\n      <td>0.894</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Therefore, we need to switch to a model-based ...</td>\n      <td>因此，我们需要切换到基于模型的方法。</td>\n      <td>0.931</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Also since we aim to do the alignment for bili...</td>\n      <td>此外，由于我们的目标是对双语文本进行对齐，现在让我们从现在开始在这种情况下思考，因为同样的方...</td>\n      <td>0.916</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>In order to achieve this, we make use of pre-t...</td>\n      <td>为了实现这一点，我们利用预训练的编码器-解码器模型将这些句子转换为向量表示，然后使用这些向量...</td>\n      <td>0.905</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>On further search, we shortlisted two models, ...</td>\n      <td>在进一步搜索中，我们入围了两个模型，LASER（来自 Facebook）和 LaBSE（来自...</td>\n      <td>0.973</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>In this article, Vishal Mahuli explains the co...</td>\n      <td>在本文中，Vishal Mahuli 解释了它们之间的比较。</td>\n      <td>0.927</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Since LaBSE gave considerably better outputs f...</td>\n      <td>由于 LaBSE 为印度语言提供了更好的输出，因此它被用作 Anuvaad 项目 (http...</td>\n      <td>0.928</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Basic Implementation</td>\n      <td>基本实现</td>\n      <td>0.878</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Generate vector embeddings for all the sentenc...</td>\n      <td>使用 LaBSE 为源语言和目标语言句子的所有句子生成向量嵌入。</td>\n      <td>0.917</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Calculate the cosine distances between each of...</td>\n      <td>计算每个句子对之间的余弦距离，具有最大余弦相似度的句子对将被视为一对。</td>\n      <td>0.817</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "model.match(en_result_list,ch_result_list)\n",
    "df=model.get_matches()\n",
    "df"
   ]
  },
  {
   "source": [
    "为计算只含部分信息的译文，相似度上的均值和标准差\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['本文概述了句子对齐过程以及如何使用可扩展的管道构建它们',\n",
       " '我们还讨论了作为',\n",
       " 'Project',\n",
       " 'Anuvaad（一个基于印度语的翻译系统）的一部分在执行大规模对齐方面所面临的挑战，以及如何使用',\n",
       " 'FAISS',\n",
       " '对其进行优化以显着提高双语语料库的管理性能',\n",
       " '我们是印度最大的公开平行语料库',\n",
       " 'samanantar',\n",
       " '的最大贡献者之一',\n",
       " '我们也将成为',\n",
       " 'ULCA（通用语言贡献',\n",
       " 'API）系统的早期贡献者',\n",
       " '双语句子对齐',\n",
       " '双语句子对齐（或跨语言语义相似性）被定义为搜索两个可用的句子集合并在其中识别有意义匹配的句子对（如果有）',\n",
       " '首先，要详细了解整个过程，让我们从基础开始',\n",
       " '考虑两个具有相似（但不完全匹配）句子的列表',\n",
       " 'aligner',\n",
       " '背后的想法是识别“我的名字是阿斯温”和“阿斯温，那是我的名字”这样的情况都是相似的',\n",
       " '尽管问题乍一看似乎很复杂，但有许多方法可以实现这一目标',\n",
       " '可以使用',\n",
       " 'Fuzzywuzzy、Word2vec、TF-IDF',\n",
       " '等基本方法来实现相同的目的',\n",
       " '不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论',\n",
       " '但是，这里的想法是要确定如果语言对是英语-英语，这样做相对简单',\n",
       " '可以参考这篇文章来详细了解这里的各种方法',\n",
       " '下面的代码解释了一种在语言为',\n",
       " 'en-en',\n",
       " '时查找匹配对的简单方法',\n",
       " '即使这种简单的方法在基本情况下有效，我们也不能指望它在复杂和模棱两可的情况下提供理想的输出',\n",
       " '因此，我们需要切换到基于模型的方法',\n",
       " '此外，由于我们的目标是对双语文本进行对齐，现在让我们从现在开始在这种情况下思考，因为同样的方法也可以应用于单语案例',\n",
       " '为了实现这一点，我们利用预训练的编码器-解码器模型将这些句子转换为向量表示，然后使用这些向量表示（称为嵌入）来计算句子之间的相似度',\n",
       " '在进一步搜索中，我们入围了两个模型，LASER（来自',\n",
       " 'Facebook）和',\n",
       " 'LaBSE（来自',\n",
       " 'Google）',\n",
       " '在本文中，Vishal',\n",
       " 'Mahuli',\n",
       " '解释了它们之间的比较',\n",
       " '由于',\n",
       " 'LaBSE',\n",
       " '为印度语言提供了更好的输出，因此它被用作',\n",
       " 'Anuvaad',\n",
       " '项目',\n",
       " '(https://anuvaad.org/)',\n",
       " '的一部分',\n",
       " '基本实现',\n",
       " '使用',\n",
       " 'LaBSE',\n",
       " '为源语言和目标语言句子的所有句子生成向量嵌入',\n",
       " '计算每个句子对之间的余弦距离，具有最大余弦相似度的句子对将被视为一对',\n",
       " '这是一种使用包装库',\n",
       " 'polyfuzz',\n",
       " '的基本方法，它计算',\n",
       " 'BERT',\n",
       " '嵌入并基于它进行匹配',\n",
       " '输出如下所示',\n",
       " '没有为此图像提供替代文字',\n",
       " '我们在没有',\n",
       " 'polyfuzz',\n",
       " '的情况下手动尝试了相同的方法，并使用各种参数微调了方法和阈值，并以稍微不同的方式实现了相同的想法，可以在这里找到',\n",
       " '全部完成',\n",
       " '问题解决了，我们开始使用',\n",
       " 'LaBSE',\n",
       " '在各种印度语言中获得有意义的配对',\n",
       " '然而，系统性能受到了影响，处理时间随着输入数据呈指数增长',\n",
       " '当输入句子数超过',\n",
       " '100K',\n",
       " '时，它甚至需要几天时间',\n",
       " '这导致了一种在不影响输出质量的情况下微调对齐的方法',\n",
       " '最初，尝试了基本方法，例如将模型保存到本地并加载并一次又一次地使用它，在',\n",
       " 'GPU',\n",
       " '上运行嵌入计算等',\n",
       " '通过切换到',\n",
       " 'GPU，使用',\n",
       " 'LaBSE',\n",
       " '进行矢量化的时间显示出显着差异，但是，余弦相似度计算和蛮力匹配方法仍然很耗时',\n",
       " '文件末尾列出了注明的时间',\n",
       " '总体时间减少到一半，但是，当数据集大小增加时，它仍然缓慢并且时间呈指数增长',\n",
       " '然后，我们搜索并尝试了多种方法来实现向量之间的有效相似性搜索，并在',\n",
       " 'KNN',\n",
       " '和',\n",
       " 'FAISS',\n",
       " '中落地',\n",
       " 'K',\n",
       " '最近邻',\n",
       " 'KNN',\n",
       " '的工作原理是找出查询与数据中所有示例之间的距离，选择最接近查询的指定数量的示例',\n",
       " '(K)，然后投票选出最频繁的标签（在分类的情况下）或平均标签',\n",
       " '(在回归的情况下）',\n",
       " 'KNN',\n",
       " '也可用于高效搜索',\n",
       " 'KNN',\n",
       " '算法',\n",
       " '1.加载数据',\n",
       " '2.',\n",
       " '将',\n",
       " 'K',\n",
       " '初始化为您选择的邻居数',\n",
       " '3.对于数据中的每个例子',\n",
       " '从数据中计算查询示例与当前示例之间的距离',\n",
       " '将示例的距离和索引添加到有序集合中',\n",
       " '4.',\n",
       " '按距离升序对有序的距离和索引集合进行排序',\n",
       " '5.',\n",
       " '从已排序的集合中挑选前',\n",
       " 'K',\n",
       " '个条目',\n",
       " '6.',\n",
       " '获取选中的K个条目的标签',\n",
       " '7.',\n",
       " '如果需要，执行回归/分类',\n",
       " '在这里，我们不使用',\n",
       " 'KNN',\n",
       " '进行任何预测，而仅用于相似性搜索',\n",
       " '有多种计算距离的方法，根据我们要解决的问题，一种方法可能更可取',\n",
       " 'KNN',\n",
       " '的主要缺点是随着输入样本数量的增加，算法变得明显变慢',\n",
       " '这就是',\n",
       " 'FAISS',\n",
       " '发挥作用的地方',\n",
       " '法斯',\n",
       " 'FAISS（Facebook',\n",
       " 'AI',\n",
       " 'Similarity',\n",
       " 'Search）是Facebook开发的一个用于高效相似度搜索和密集向量聚类的库',\n",
       " 'FAISS',\n",
       " '是用',\n",
       " 'C++',\n",
       " '编写的，带有',\n",
       " 'Python/numpy',\n",
       " '的完整包装器',\n",
       " '它包含在任意大小的向量集中搜索的算法，直到那些可能不适合',\n",
       " 'RAM',\n",
       " '的向量',\n",
       " '一些最有用的算法在',\n",
       " 'GPU',\n",
       " '上实现以提高性能',\n",
       " 'FAISS',\n",
       " '是围绕一种索引类型构建的，该索引类型存储了一组向量，并提供了一个函数来通过',\n",
       " 'L2',\n",
       " '和/或点积向量比较在其中进行搜索',\n",
       " '一些索引类型是简单的基线，例如精确搜索',\n",
       " 'FAISS',\n",
       " '经过调整，与现有的库（如',\n",
       " 'Scikit',\n",
       " 'Learn）相比，它极大地缩小了向量',\n",
       " 'k',\n",
       " '最近邻的搜索空间，以便在向量之间进行更快的相似性搜索',\n",
       " '这种技术被称为近似最近邻',\n",
       " '(ANN)',\n",
       " '搜索，并牺牲了一些精度来获得巨大的加速',\n",
       " '初步准备',\n",
       " '我们已经对',\n",
       " 'FAISS',\n",
       " '和其中的各种索引进行了几次实验',\n",
       " '但是，以下是所有人都通用的一些准备步骤',\n",
       " '初始化模型并基于它对嵌入进行编码',\n",
       " '这些嵌入被传递给',\n",
       " 'KNN',\n",
       " '函数，该函数根据可以重新创建的匹配数据集返回相似度分数和相应的列表索引',\n",
       " '没有为此图像提供替代文字',\n",
       " 'FAISS',\n",
       " '指数',\n",
       " 'FAISS',\n",
       " '是围绕',\n",
       " 'Index',\n",
       " '对象构建的',\n",
       " '它封装了一组数据库向量，并可选择对它们进行预处理以提高搜索效率',\n",
       " '有许多类型的索引',\n",
       " '平面索引只是将向量编码为固定大小的代码，并将它们存储在',\n",
       " 'ntotal',\n",
       " '*',\n",
       " 'code_size',\n",
       " '字节的数组中',\n",
       " '在搜索时，所有索引向量都被顺序解码并与查询向量进行比较',\n",
       " '在平面索引',\n",
       " '(IndexFlat)',\n",
       " '中：向量存储时不进行压缩',\n",
       " 'FAISS',\n",
       " '中的另一类索引是基于分区的索引，它通过将索引划分为集群并将搜索限制在几个集群中来加速搜索',\n",
       " '然而，这种方法并不准确，因为不能保证最近的邻居会在搜索的集群中',\n",
       " '使用分区技术使搜索空间更小、更高效的索引示例是',\n",
       " 'IndexIVFFlat',\n",
       " '索引',\n",
       " '搜索操作可以按照与早期索引相同的方式进行',\n",
       " '但是，在',\n",
       " 'IVFFlat',\n",
       " '索引中，我们定义了“nprobe”超参数，以将搜索限制为仅与查询向量最近的定义数量的集群',\n",
       " '这也是如何将不同的索引组合成单个索引的示例',\n",
       " '下面给出了我们尝试过的索引的表格表示',\n",
       " '实验一：人工神经网络',\n",
       " '由于速度是与基本实现的主要权衡，我们最初的方法是尝试在执行速度方面提供最佳性能的方法',\n",
       " '以失去找到最近邻居的保证为代价来加速该过程的典型方法是采用分区技术，例如',\n",
       " 'k-means',\n",
       " '相应的算法有时被称为细胞探测方法',\n",
       " '我们使用基于多探测的基于分区的方法',\n",
       " '特征空间被划分为',\n",
       " 'nlist',\n",
       " '单元',\n",
       " '使用量化函数将数据库向量分配给这些单元之一（在',\n",
       " 'k-means',\n",
       " '的情况下，分配给最接近查询的质心），并存储在由',\n",
       " 'nlist',\n",
       " '倒排列表形成的倒排文件结构中',\n",
       " '查询时，选择一组nprobe倒排列表',\n",
       " '将查询与分配给这些列表的每个数据库向量进行比较',\n",
       " '这样做，只有一部分数据库与查询进行比较：作为第一个近似值，这个部分是',\n",
       " 'nprobe/nlist，但是这个近似值通常被低估，因为倒排列表的长度不相等',\n",
       " '当未选择给定查询的最近邻居的单元格时会出现失败情况',\n",
       " '构造函数将索引作为参数（量化器或粗量化器），用于对倒排列表进行赋值',\n",
       " '在该索引中搜索查询，返回的向量',\n",
       " 'id(s)',\n",
       " '是应该访问的倒排列表',\n",
       " '没有为此图像提供替代文字',\n",
       " '带走',\n",
       " '超快执行',\n",
       " '输出足够好，但对于并行语料库数据集的创建来说质量不佳',\n",
       " '无论速度如何，基本实现都能提供更好的结果',\n",
       " '实验',\n",
       " '2：余弦相似度测量',\n",
       " '由于基于余弦相似度的初始基本实现比基于',\n",
       " 'ANN',\n",
       " '的方法提供了更好的质量输出，接下来的想法是尝试一下',\n",
       " '在',\n",
       " 'FAISS',\n",
       " '中，我们没有余弦相似度方法，但有计算向量之间的内积或点积的索引',\n",
       " '然后我们可以利用余弦相似度只是归一化向量之间的点积这一事实',\n",
       " '下面的代码片段显示了如何实现这一点',\n",
       " '没有为此图像提供替代文字',\n",
       " '带走',\n",
       " '正如预期的那样，搜索比以前的方法慢一点',\n",
       " '产出质量有所提高，但仍达不到标准',\n",
       " '当阈值降低时，质量会显着降低',\n",
       " '基本实现仍然提供了更好的输出']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "def split_into_ch_sentences(text):\n",
    "    '''！！！此函数为了构造只含有部分信息的译文'''\n",
    "    import re\n",
    "    ch_result_list = re.split(r'[。！？……\\s]', ch_sentences)\n",
    "    #加了\\s所有的空格的地方都会分割，为了构造出\n",
    "    while '' in ch_result_list:\n",
    "        ch_result_list.remove('')#确保放进polyfuzz的model的句子中没有空字符串\n",
    "    '''\n",
    "    如果句子列表中有字符串为空，会报\n",
    "    141 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
    "    '''\n",
    "    return ch_result_list\n",
    "ch_sentences = read_text('c:\\\\Users\\\\zhouy217\\\\OneDrive - Pfizer\\\\Documents\\\\data_pipeline\\\\data\\\\ch_material_all.txt')\n",
    "ch_result_list=split_into_ch_sentences(ch_sentences)\n",
    "ch_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "len(ch_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['The article gives an overview of the sentence alignment process and how to build them using a scalable pipeline.',\n",
       " 'We also talk about the challenges faced as part of the Project Anuvaad (an Indic based translation system) in performing alignment at scale and how it was optimized using FAISS for considerable performance improvement in curating the bi-lingual corpus.',\n",
       " 'We are one of the biggest contributors to India’s largest publicly available parallel corpus samanantar.',\n",
       " 'We will also be the early contributors to the ULCA ( Universal Language Contribution API ) system.',\n",
       " 'Bilingual Sentence Alignment',\n",
       " 'Bilingual Sentence alignment (or cross-language semantic similarity) is defined as searching through two available collections of sentences and identifying meaningfully matching sentence pairs (if any) among them.',\n",
       " 'First, to get a detailed idea about the whole process, let’s start from the basics.',\n",
       " 'Consider two lists, having similar (but not exactly matching) sentences.',\n",
       " 'The idea behind aligner is to identify cases like “My name is Aswin” and “Aswin, thats my name” are both similar.',\n",
       " 'Even Though the problem seems complex at the first glance, there are numerous methods available for achieving this.',\n",
       " 'Basic methods like Fuzzywuzzy, Word2vec, TF-IDF, etc could be used to achieve the same.',\n",
       " 'Extracting out quality pairs in complex inputs is a bit tricky though, which we will discuss later.',\n",
       " 'However, the idea here is to establish the fact that it’s relatively simple to do it if the language pair is English-English.',\n",
       " 'One can refer to this article to understand varied approaches in detail here.',\n",
       " 'The code below explains a simple approach to find matching pairs if language is en-en.',\n",
       " 'Even if such simple approaches work out in basic situations, we cannot expect it to provide ideal output in complex and ambiguous cases.',\n",
       " 'Therefore, we need to switch to a model-based approach.',\n",
       " 'Also since we aim to do the alignment for bilingual texts, now let’s think in that context from now on, since the same approach could be applied for the monolingual cases as well.',\n",
       " 'In order to achieve this, we make use of pre-trained encoder-decoder models that transform these sentences into vector representations, these vector representations (known as embeddings) are then used to calculate the similarity between the sentences.',\n",
       " 'On further search, we shortlisted two models, LASER (from Facebook) and LaBSE (from Google).',\n",
       " 'In this article, Vishal Mahuli explains the comparison between both of them.',\n",
       " 'Since LaBSE gave considerably better outputs for Indic languages, it was used as part of the Anuvaad project (https://anuvaad.org/).',\n",
       " 'Basic Implementation',\n",
       " 'Generate vector embeddings for all the sentences for source and target languages sentences with LaBSE.',\n",
       " 'Calculate the cosine distances between each of these sentence pairs and the couple that has the largest cosine similarity will be considered to be a pair.',\n",
       " 'This is a basic approach using the wrapper library polyfuzz which calculates BERT embeddings and does matching based on it.',\n",
       " 'The output looks as below.',\n",
       " 'No alt text provided for this image',\n",
       " 'We tried the same manually without polyfuzz and fine-tuned the approach and threshold using various parameters and implemented the same idea in a slightly different manner, which could be found here.',\n",
       " 'All Done?',\n",
       " 'Problem solved and we started getting meaningful pairs using LaBSE across various Indian languages.',\n",
       " 'However, the system took a performance hit and processing time was increasing exponentially with input data.',\n",
       " 'It even used to take days when the input sentences count was exceeding 100K.',\n",
       " 'This led to a way to fine-tune the alignment without compromising on the quality of the output.',\n",
       " 'Initially, basic approaches were tried out, like saving the model to local and loading it and using it again and again, running embedding calculations on GPU, etc.',\n",
       " 'By switching to GPU, the time for vectorization using LaBSE showed notable differences, however, cosine similarity calculation and the brute force matching approach were still time-consuming.',\n",
       " 'Noted timings are tabulated towards the end of the document.',\n",
       " 'Overall timings were reduced to half, however, it was still sluggish and time increased exponentially when dataset size increases.',\n",
       " 'Then we searched upon and experimented with a wide range of approaches for efficient similarity search between vectors and landed up in KNN and FAISS.',\n",
       " 'K Nearest Neighbour',\n",
       " 'KNN works by finding the distances between a query and all the examples in the data, selecting the specified number of examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).',\n",
       " 'KNN could be used for efficient searching as well.',\n",
       " 'KNN Algorithm',\n",
       " '1.',\n",
       " 'Load the data',\n",
       " '2.',\n",
       " 'Initialize K to your chosen number of neighbours',\n",
       " '3.',\n",
       " 'For each example in the data',\n",
       " 'Calculate the distance between the query example and the current example from the data.',\n",
       " 'Add the distance and the index of the example to an ordered collection.',\n",
       " '4.',\n",
       " 'Sort the ordered collection of distances and indices in ascending order by the distances.',\n",
       " '5.',\n",
       " 'Pick the first K entries from the sorted collection',\n",
       " '6.',\n",
       " 'Get the labels of the selected K entries',\n",
       " '7.',\n",
       " 'Perform Regression/Classification if needed',\n",
       " 'Here, we use KNN not for any prediction, but just for similarity searching.',\n",
       " 'There are different ways of calculating distance, and one way might be preferable depending on the problem we are solving.',\n",
       " 'The major drawback of KNN is that The algorithm gets significantly slower as the number of input samples increases.',\n",
       " 'That’s where FAISS comes into the picture.',\n",
       " 'FAISS',\n",
       " 'FAISS (Facebook AI Similarity Search) is a library developed by Facebook for efficient similarity search and clustering of dense vectors.',\n",
       " 'FAISS is written in C++ with complete wrappers for Python/numpy.',\n",
       " 'It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.',\n",
       " 'Some of the most useful algorithms are implemented on the GPU to add up to the performance.',\n",
       " 'FAISS is built around an index type that stores a set of vectors and provides a function to search in them with L2 and/or dot product vector comparison.',\n",
       " 'Some index types are simple baselines, such as exact search.',\n",
       " 'FAISS is tuned such that it vastly narrows down the search space for a vector’s k-nearest neighbors to have a much faster similarity search between vectors as compared to existing libraries like Scikit Learn.',\n",
       " 'This technique is called Approximate Nearest Neighbours (ANN) search and sacrifices some precision to obtain vast speedups.',\n",
       " 'Initial Preparation',\n",
       " 'We have been doing several experiments with FAISS and various indexes within it.',\n",
       " 'However, the following are some preparatory steps that are common for all.',\n",
       " 'A model is initialized and embeddings are encoded based on it.',\n",
       " 'These embeddings are passed to the KNN function, which returns the similarity score and corresponding list index based on which matched dataset could be recreated.',\n",
       " 'No alt text provided for this image',\n",
       " 'FAISS Indexes',\n",
       " 'FAISS is built around the Index object.',\n",
       " 'It encapsulates the set of database vectors and optionally preprocesses them to make searching efficient.',\n",
       " 'There are many types of indexes.',\n",
       " 'Flat indexes just encode the vectors into codes of a fixed size and store them in an array of ntotal * code_size bytes.',\n",
       " 'At search time, all the indexed vectors are decoded sequentially and compared to the query vectors.',\n",
       " 'In Flat Indexing (IndexFlat): the vectors are stored without compression.',\n",
       " 'Another class of indexes in FAISS are partition-based indexes that speed up searches by partitioning the index into clusters and limiting the search to only a few clusters.',\n",
       " 'This method however is not exact as there is no guarantee that the nearest neighbors will be in the clusters searched in.',\n",
       " 'An example of an index that uses partitioning techniques to make the search space a lot less and far more efficient is IndexIVFFlat index.',\n",
       " 'The search operation can be carried out in the same way as earlier indexes.',\n",
       " 'However, in the IVFFlat index we define the “nprobe” hyperparameter to limit the search to only the defined number of clusters nearest to the query vector.',\n",
       " 'This is also an example of how different indexes can be compounded to form a single index.',\n",
       " 'Given below is a tabular representation of indexes we have tried out',\n",
       " 'Experiment 1 : ANN',\n",
       " 'Since speed was the major trade-off with the basic implementation, our initial approach was to try out the method which offers the best performance in terms of execution speed.',\n",
       " 'A typical way to speed up the process at the cost of losing the guarantee to find the nearest neighbor is to employ a partitioning technique such as k-means.',\n",
       " 'The corresponding algorithms are sometimes referred to as cell-probe methods.',\n",
       " 'We use a partition-based method based on Multi-probing.',\n",
       " 'The feature space is partitioned into nlist cells.',\n",
       " 'The database vectors are assigned to one of these cells using a quantization function (in the case of k-means, the assignment to the centroid closest to the query), and stored in an inverted file structure formed of nlist inverted lists.',\n",
       " 'At query time, a set of nprobe inverted lists is selected',\n",
       " 'The query is compared to each of the database vectors assigned to these lists.',\n",
       " 'Doing so, only a fraction of the database is compared to the query: as a first approximation, this fraction is nprobe/nlist, but this approximation is usually under-estimated because the inverted lists have not equal lengths.',\n",
       " 'The failure case appears when the cell of the nearest neighbor of a given query is not selected.',\n",
       " 'The constructor takes an index as a parameter (the quantizer or coarse quantizer), which is used to do the assignment to the inverted lists.',\n",
       " 'The query is searched in this index, and the returned vector id(s) are the inverted list(s) that should be visited.',\n",
       " 'No alt text provided for this image',\n",
       " 'Takeaway',\n",
       " 'Superfast execution.',\n",
       " 'Output is decent enough but not good quality for parallel corpus dataset creation.',\n",
       " 'Basic implementation gave better results regardless of speed.',\n",
       " 'Experiment 2 : Cosine Similarity Measurement',\n",
       " 'Since the initial basic implementation based on cosine similarity gave far better quality output than that of the ANN based approach, the very next thought is to give it a try with that.',\n",
       " 'In FAISS we don’t have a cosine similarity method but have indexes that calculate the inner or dot product between vectors.',\n",
       " 'We can then take advantage of the fact that cosine similarity is simply the dot product between normalized vectors.',\n",
       " 'The code snippet below shows how this can be implemented.',\n",
       " 'No alt text provided for this image',\n",
       " 'Takeaway',\n",
       " 'As expected, the search is a bit slower than the previous method.',\n",
       " 'Quality of output increased but still not up to the mark.',\n",
       " 'Quality decreases considerably as and when the threshold decreases.',\n",
       " 'Basic implementation still gave better output.']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "en_sentences = read_text('c:\\\\Users\\\\zhouy217\\\\OneDrive - Pfizer\\\\Documents\\\\data_pipeline\\\\data\\\\en_material_entire_paragraphs.txt')\n",
    "en_result_list=split_into_en_sentences_modified_1(en_sentences)\n",
    "en_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  From  \\\n",
       "0    The article gives an overview of the sentence ...   \n",
       "1    We also talk about the challenges faced as par...   \n",
       "2    We are one of the biggest contributors to Indi...   \n",
       "3    We will also be the early contributors to the ...   \n",
       "4                         Bilingual Sentence Alignment   \n",
       "..                                                 ...   \n",
       "116                                           Takeaway   \n",
       "117  As expected, the search is a bit slower than t...   \n",
       "118  Quality of output increased but still not up t...   \n",
       "119  Quality decreases considerably as and when the...   \n",
       "120     Basic implementation still gave better output.   \n",
       "\n",
       "                                                    To  Similarity  \n",
       "0                         本文概述了句子对齐过程以及如何使用可扩展的管道构建它们。       0.867  \n",
       "1    我们还讨论了作为 Project Anuvaad（一个基于印度语的翻译系统）的一部分在执行大...       0.923  \n",
       "2                 我们是印度最大的公开平行语料库 samanantar 的最大贡献者之一。       0.840  \n",
       "3                     我们也将成为 ULCA（通用语言贡献 API）系统的早期贡献者。       0.959  \n",
       "4                                               双语句子对齐       0.806  \n",
       "..                                                 ...         ...  \n",
       "116                                               基本实现       0.232  \n",
       "117                       尽管问题乍一看似乎很复杂，但有许多方法可以实现这一目标。       0.315  \n",
       "118                       不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论。       0.331  \n",
       "119                       不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论。       0.243  \n",
       "120     即使这种简单的方法在基本情况下有效，我们也不能指望它在复杂和模棱两可的情况下提供理想的输出。       0.434  \n",
       "\n",
       "[121 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>From</th>\n      <th>To</th>\n      <th>Similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The article gives an overview of the sentence ...</td>\n      <td>本文概述了句子对齐过程以及如何使用可扩展的管道构建它们。</td>\n      <td>0.867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>We also talk about the challenges faced as par...</td>\n      <td>我们还讨论了作为 Project Anuvaad（一个基于印度语的翻译系统）的一部分在执行大...</td>\n      <td>0.923</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>We are one of the biggest contributors to Indi...</td>\n      <td>我们是印度最大的公开平行语料库 samanantar 的最大贡献者之一。</td>\n      <td>0.840</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>We will also be the early contributors to the ...</td>\n      <td>我们也将成为 ULCA（通用语言贡献 API）系统的早期贡献者。</td>\n      <td>0.959</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bilingual Sentence Alignment</td>\n      <td>双语句子对齐</td>\n      <td>0.806</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>Takeaway</td>\n      <td>基本实现</td>\n      <td>0.232</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>As expected, the search is a bit slower than t...</td>\n      <td>尽管问题乍一看似乎很复杂，但有许多方法可以实现这一目标。</td>\n      <td>0.315</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>Quality of output increased but still not up t...</td>\n      <td>不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论。</td>\n      <td>0.331</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>Quality decreases considerably as and when the...</td>\n      <td>不过，在复杂输入中提取质量对有点棘手，我们将在后面讨论。</td>\n      <td>0.243</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>Basic implementation still gave better output.</td>\n      <td>即使这种简单的方法在基本情况下有效，我们也不能指望它在复杂和模棱两可的情况下提供理想的输出。</td>\n      <td>0.434</td>\n    </tr>\n  </tbody>\n</table>\n<p>121 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "model.match(en_result_list,ch_result_list)\n",
    "df=model.get_matches()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Similarity', ylabel='Density'>"
      ]
     },
     "metadata": {},
     "execution_count": 44
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"385.78125pt\" height=\"262.19625pt\" viewBox=\"0 0 385.78125 262.19625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-09-07T09:20:46.566279</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 385.78125 262.19625 \nL 385.78125 0 \nL 0 0 \nL 0 262.19625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 224.64 \nL 378.58125 224.64 \nL 378.58125 7.2 \nL 43.78125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 120.565662 224.64 \nL 135.66826 224.64 \nL 135.66826 125.238857 \nL 120.565662 125.238857 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 135.66826 224.64 \nL 150.770858 224.64 \nL 150.770858 42.404571 \nL 135.66826 42.404571 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 150.770858 224.64 \nL 165.873456 224.64 \nL 165.873456 17.554286 \nL 150.770858 17.554286 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 165.873456 224.64 \nL 180.976054 224.64 \nL 180.976054 42.404571 \nL 165.873456 42.404571 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 180.976054 224.64 \nL 196.078652 224.64 \nL 196.078652 116.955429 \nL 180.976054 116.955429 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 196.078652 224.64 \nL 211.18125 224.64 \nL 211.18125 224.64 \nL 196.078652 224.64 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 211.18125 224.64 \nL 226.283848 224.64 \nL 226.283848 208.073143 \nL 211.18125 208.073143 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 226.283848 224.64 \nL 241.386446 224.64 \nL 241.386446 224.64 \nL 226.283848 224.64 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 241.386446 224.64 \nL 256.489044 224.64 \nL 256.489044 224.64 \nL 241.386446 224.64 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 256.489044 224.64 \nL 271.591642 224.64 \nL 271.591642 183.222857 \nL 256.489044 183.222857 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 271.591642 224.64 \nL 286.69424 224.64 \nL 286.69424 158.372571 \nL 271.591642 158.372571 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 286.69424 224.64 \nL 301.796838 224.64 \nL 301.796838 125.238857 \nL 286.69424 125.238857 \nz\n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: #1f77b4; opacity: 0.4\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"ma89f66020e\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma89f66020e\" x=\"77.733389\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(69.781827 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#ma89f66020e\" x=\"123.789597\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <g transform=\"translate(115.838034 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#ma89f66020e\" x=\"169.845804\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <g transform=\"translate(161.894241 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#ma89f66020e\" x=\"215.902011\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <g transform=\"translate(207.950449 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#ma89f66020e\" x=\"261.958219\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <g transform=\"translate(254.006656 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#ma89f66020e\" x=\"308.014426\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <g transform=\"translate(300.062863 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#ma89f66020e\" x=\"354.070633\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1.2 -->\n      <g transform=\"translate(346.119071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Similarity -->\n     <g transform=\"translate(187.541406 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-53\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"63.476562\"/>\n      <use xlink:href=\"#DejaVuSans-6d\" x=\"91.259766\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"188.671875\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"216.455078\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"244.238281\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"305.517578\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"346.630859\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"374.414062\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"413.623047\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"m31f05acb07\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m31f05acb07\" x=\"43.78125\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m31f05acb07\" x=\"43.78125\" y=\"191.773081\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.5 -->\n      <g transform=\"translate(20.878125 195.5723)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m31f05acb07\" x=\"43.78125\" y=\"158.906162\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 162.705381)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m31f05acb07\" x=\"43.78125\" y=\"126.039243\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.5 -->\n      <g transform=\"translate(20.878125 129.838462)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m31f05acb07\" x=\"43.78125\" y=\"93.172325\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2.0 -->\n      <g transform=\"translate(20.878125 96.971543)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m31f05acb07\" x=\"43.78125\" y=\"60.305406\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 2.5 -->\n      <g transform=\"translate(20.878125 64.104624)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m31f05acb07\" x=\"43.78125\" y=\"27.438487\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 3.0 -->\n      <g transform=\"translate(20.878125 31.237706)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Density -->\n     <g transform=\"translate(14.798438 134.928594)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-44\" d=\"M 1259 4147 \nL 1259 519 \nL 2022 519 \nQ 2988 519 3436 956 \nQ 3884 1394 3884 2338 \nQ 3884 3275 3436 3711 \nQ 2988 4147 2022 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 1925 4666 \nQ 3281 4666 3915 4102 \nQ 4550 3538 4550 2338 \nQ 4550 1131 3912 565 \nQ 3275 0 1925 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-44\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"77.001953\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"138.525391\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"201.904297\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"254.003906\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"281.787109\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"320.996094\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 58.999432 224.52425 \nL 66.646759 224.258628 \nL 71.235156 223.903804 \nL 75.823552 223.278034 \nL 78.882483 222.63478 \nL 81.941414 221.741532 \nL 85.000345 220.52582 \nL 88.059276 218.904011 \nL 89.588742 217.912049 \nL 91.118208 216.783116 \nL 92.647673 215.504655 \nL 94.177139 214.064011 \nL 95.706604 212.448614 \nL 97.23607 210.646187 \nL 98.765535 208.64497 \nL 100.295001 206.433965 \nL 101.824466 204.00319 \nL 103.353932 201.343945 \nL 104.883397 198.449079 \nL 107.942328 191.9332 \nL 111.001259 184.439111 \nL 114.06019 175.990594 \nL 117.119121 166.657036 \nL 120.178052 156.556213 \nL 124.766449 140.341082 \nL 133.943242 106.986383 \nL 137.002173 96.597869 \nL 140.061104 87.042292 \nL 143.120035 78.575815 \nL 144.6495 74.822067 \nL 146.178966 71.420805 \nL 147.708431 68.393063 \nL 149.237897 65.756821 \nL 150.767362 63.526911 \nL 152.296828 61.714979 \nL 153.826293 60.329489 \nL 155.355759 59.375783 \nL 156.885224 58.856168 \nL 158.41469 58.770035 \nL 159.944155 59.113999 \nL 161.473621 59.882057 \nL 163.003086 61.065743 \nL 164.532552 62.654289 \nL 166.062017 64.634781 \nL 167.591483 66.992303 \nL 169.120948 69.710067 \nL 170.650414 72.769544 \nL 172.17988 76.150569 \nL 175.238811 83.789098 \nL 178.297742 92.435782 \nL 181.356673 101.881744 \nL 185.945069 117.058138 \nL 195.121862 148.175934 \nL 198.180793 157.975788 \nL 201.239724 167.182885 \nL 204.298655 175.661094 \nL 207.357586 183.311626 \nL 210.416517 190.072924 \nL 213.475448 195.917514 \nL 215.004914 198.495428 \nL 216.534379 200.846372 \nL 218.063845 202.973729 \nL 219.59331 204.881723 \nL 221.122776 206.575184 \nL 222.652241 208.059325 \nL 224.181707 209.33955 \nL 225.711172 210.421276 \nL 227.240638 211.309804 \nL 228.770103 212.010207 \nL 230.299569 212.527266 \nL 231.829034 212.865438 \nL 233.3585 213.028856 \nL 234.887965 213.021368 \nL 236.417431 212.846598 \nL 237.946896 212.508045 \nL 239.476362 212.009197 \nL 241.005827 211.353663 \nL 242.535293 210.545326 \nL 244.064758 209.588498 \nL 245.594224 208.488084 \nL 248.653155 205.880043 \nL 251.712086 202.778248 \nL 254.771017 199.258518 \nL 257.829948 195.418441 \nL 270.065672 179.56152 \nL 273.124603 176.283655 \nL 274.654069 174.863564 \nL 276.183534 173.614853 \nL 277.713 172.554635 \nL 279.242465 171.697997 \nL 280.771931 171.057671 \nL 282.301396 170.643743 \nL 283.830862 170.463417 \nL 285.360327 170.520825 \nL 286.889793 170.816907 \nL 288.419258 171.349366 \nL 289.948724 172.11268 \nL 291.478189 173.098205 \nL 293.007655 174.294334 \nL 294.53712 175.686737 \nL 296.066586 177.258651 \nL 299.125517 180.863937 \nL 302.184448 184.941646 \nL 306.772844 191.546053 \nL 312.890706 200.348532 \nL 315.949637 204.419552 \nL 319.008568 208.130845 \nL 322.067499 211.420416 \nL 325.12643 214.259082 \nL 328.185361 216.646286 \nL 331.244292 218.604274 \nL 334.303224 220.171576 \nL 337.362155 221.396594 \nL 340.421086 222.331923 \nL 343.480017 223.02979 \nL 346.538948 223.538764 \nL 351.127344 224.040002 \nL 357.245206 224.391039 \nL 363.363068 224.544662 \nL 363.363068 224.544662 \n\" clip-path=\"url(#p5f1ca2e575)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 43.78125 224.64 \nL 43.78125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 378.58125 224.64 \nL 378.58125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path d=\"M 43.78125 224.64 \nL 378.58125 224.64 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 43.78125 7.2 \nL 378.58125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5f1ca2e575\">\n   <rect x=\"43.78125\" y=\"7.2\" width=\"334.8\" height=\"217.44\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(df['Similarity'])\n",
    "#从下图可以看出，只包含部分信息的句子，匹配相似度较低；可以考虑设置阈值为0.6用来筛除只包含部分信息的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4660495867768595"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "df['Similarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.232552067690932"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "df['Similarity'].std()"
   ]
  },
  {
   "source": [
    "#### polyfuzz的使用，以及匹配规则\n",
    "polyfuzz官网https://maartengr.github.io/PolyFuzz/\n",
    "\n",
    "匹配以source_sentences（.match()的第一个参数）为准，见下面示例\n",
    "\n",
    "句子数不等时，可以考虑将句子数少的句子列表放在第一个参数中"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ANACONDA3\\envs\\ja_nlp_python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                From                    To  \\\n",
       "0                                     my name is Tom  Tom, that is my name   \n",
       "1  The main ingredient of this product is flucona...                  None   \n",
       "2                                         I love NLP                  None   \n",
       "\n",
       "   Similarity  \n",
       "0       0.703  \n",
       "1       0.000  \n",
       "2       0.000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>From</th>\n      <th>To</th>\n      <th>Similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>my name is Tom</td>\n      <td>Tom, that is my name</td>\n      <td>0.703</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The main ingredient of this product is flucona...</td>\n      <td>None</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I love NLP</td>\n      <td>None</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from polyfuzz import PolyFuzz\n",
    "source_sentences = ['my name is Tom','The main ingredient of this product is fluconazole','I love NLP']\n",
    "target_sentences = ['Tom, that is my name','本品主要成份为氟康唑']\n",
    "model = PolyFuzz('TF-IDF')#选用tfidf只适用于英语\n",
    "model.match(source_sentences,target_sentences)\n",
    "model.get_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from polyfuzz import PolyFuzz\n",
    "from polyfuzz.models import Embeddings\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.embeddings import SentenceTransformerDocumentEmbeddings\n",
    "embeddings = SentenceTransformerDocumentEmbeddings('LaBSE')\n",
    "LaBSE = Embeddings(embeddings,min_similarity=0,model_id='LaBSE')\n",
    "model=PolyFuzz([LaBSE])"
   ]
  },
  {
   "source": [
    "source sentences与target sentence一样多"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                From  \\\n",
       "0  The most frequently (>1/10) reported adverse r...   \n",
       "1  The main ingredient of this product is flucona...   \n",
       "2  Reporting suspected adverse reactions after au...   \n",
       "3  It allows continued monitoring of the benefit/...   \n",
       "4  Healthcare professionals are asked to report a...   \n",
       "\n",
       "                                                  To  Similarity  \n",
       "0  最常见（>1/10）报告的不良反应是头痛、腹痛、腹泻、恶心、呕吐、丙氨酸氨基转移酶升高、天冬...       0.875  \n",
       "1                                         本品主要成份为氟康唑       0.782  \n",
       "2                        疑似不良反应报告在药品批准后报告可疑的不良反应很重要。       0.885  \n",
       "3                                 它允许持续监测药品的收益/风险平衡。       0.924  \n",
       "4                             要求医疗保健专业人员报告任何可疑的不良反应。       0.892  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>From</th>\n      <th>To</th>\n      <th>Similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The most frequently (&gt;1/10) reported adverse r...</td>\n      <td>最常见（&gt;1/10）报告的不良反应是头痛、腹痛、腹泻、恶心、呕吐、丙氨酸氨基转移酶升高、天冬...</td>\n      <td>0.875</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The main ingredient of this product is flucona...</td>\n      <td>本品主要成份为氟康唑</td>\n      <td>0.782</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Reporting suspected adverse reactions after au...</td>\n      <td>疑似不良反应报告在药品批准后报告可疑的不良反应很重要。</td>\n      <td>0.885</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It allows continued monitoring of the benefit/...</td>\n      <td>它允许持续监测药品的收益/风险平衡。</td>\n      <td>0.924</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Healthcare professionals are asked to report a...</td>\n      <td>要求医疗保健专业人员报告任何可疑的不良反应。</td>\n      <td>0.892</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "source_sentences = ['The most frequently (>1/10) reported adverse reactions are headache, abdominal pain, diarrhoea, nausea, vomiting, alanine aminotransferase increased, aspartate aminotransferase increased, blood alkaline phosphatase increased and rash.','The main ingredient of this product is fluconazole','Reporting suspected adverse reactions after authorisation of the medicinal product is important.','It allows continued monitoring of the benefit/risk balance of the medicinal product.','Healthcare professionals are asked to report any suspected adverse reactions.']\n",
    "target_sentences = ['疑似不良反应报告在药品批准后报告可疑的不良反应很重要。','它允许持续监测药品的收益/风险平衡。','要求医疗保健专业人员报告任何可疑的不良反应。','最常见（>1/10）报告的不良反应是头痛、腹痛、腹泻、恶心、呕吐、丙氨酸氨基转移酶升高、天冬氨酸氨基转移酶升高、血碱性磷酸酶升高和皮疹。','本品主要成份为氟康唑']\n",
    "\n",
    "model.match(source_sentences,target_sentences)\n",
    "model.get_matches()"
   ]
  },
  {
   "source": [
    "source_sentences多，target_sentences多次出现以匹配source_sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                From  \\\n",
       "0  The most frequently (>1/10) reported adverse r...   \n",
       "1  The main ingredient of this product is flucona...   \n",
       "2  Reporting suspected adverse reactions after au...   \n",
       "3  It allows continued monitoring of the benefit/...   \n",
       "4  Healthcare professionals are asked to report a...   \n",
       "\n",
       "                            To  Similarity  \n",
       "0  疑似不良反应报告在药品批准后报告可疑的不良反应很重要。       0.264  \n",
       "1                   本品主要成份为氟康唑       0.782  \n",
       "2  疑似不良反应报告在药品批准后报告可疑的不良反应很重要。       0.885  \n",
       "3  疑似不良反应报告在药品批准后报告可疑的不良反应很重要。       0.307  \n",
       "4       要求医疗保健专业人员报告任何可疑的不良反应。       0.892  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>From</th>\n      <th>To</th>\n      <th>Similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The most frequently (&gt;1/10) reported adverse r...</td>\n      <td>疑似不良反应报告在药品批准后报告可疑的不良反应很重要。</td>\n      <td>0.264</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The main ingredient of this product is flucona...</td>\n      <td>本品主要成份为氟康唑</td>\n      <td>0.782</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Reporting suspected adverse reactions after au...</td>\n      <td>疑似不良反应报告在药品批准后报告可疑的不良反应很重要。</td>\n      <td>0.885</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It allows continued monitoring of the benefit/...</td>\n      <td>疑似不良反应报告在药品批准后报告可疑的不良反应很重要。</td>\n      <td>0.307</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Healthcare professionals are asked to report a...</td>\n      <td>要求医疗保健专业人员报告任何可疑的不良反应。</td>\n      <td>0.892</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "source_sentences = ['The most frequently (>1/10) reported adverse reactions are headache, abdominal pain, diarrhoea, nausea, vomiting, alanine aminotransferase increased, aspartate aminotransferase increased, blood alkaline phosphatase increased and rash.','The main ingredient of this product is fluconazole','Reporting suspected adverse reactions after authorisation of the medicinal product is important.','It allows continued monitoring of the benefit/risk balance of the medicinal product.','Healthcare professionals are asked to report any suspected adverse reactions.']\n",
    "target_sentences = ['疑似不良反应报告在药品批准后报告可疑的不良反应很重要。','要求医疗保健专业人员报告任何可疑的不良反应。','本品主要成份为氟康唑']\n",
    "\n",
    "model.match(source_sentences,target_sentences)\n",
    "model.get_matches()"
   ]
  },
  {
   "source": [
    "target_sentences多，选了target sentences里与source sentences匹配度高的句子做匹配"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                From                      To  \\\n",
       "0  The main ingredient of this product is flucona...              本品主要成份为氟康唑   \n",
       "1  It allows continued monitoring of the benefit/...      它允许持续监测药品的收益/风险平衡。   \n",
       "2  Healthcare professionals are asked to report a...  要求医疗保健专业人员报告任何可疑的不良反应。   \n",
       "\n",
       "   Similarity  \n",
       "0       0.782  \n",
       "1       0.924  \n",
       "2       0.892  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>From</th>\n      <th>To</th>\n      <th>Similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The main ingredient of this product is flucona...</td>\n      <td>本品主要成份为氟康唑</td>\n      <td>0.782</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>It allows continued monitoring of the benefit/...</td>\n      <td>它允许持续监测药品的收益/风险平衡。</td>\n      <td>0.924</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Healthcare professionals are asked to report a...</td>\n      <td>要求医疗保健专业人员报告任何可疑的不良反应。</td>\n      <td>0.892</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "source_sentences = ['The main ingredient of this product is fluconazole','It allows continued monitoring of the benefit/risk balance of the medicinal product.','Healthcare professionals are asked to report any suspected adverse reactions.']\n",
    "target_sentences = ['疑似不良反应报告在药品批准后报告可疑的不良反应很重要。','它允许持续监测药品的收益/风险平衡。','要求医疗保健专业人员报告任何可疑的不良反应。','最常见（>1/10）报告的不良反应是头痛、腹痛、腹泻、恶心、呕吐、丙氨酸氨基转移酶升高、天冬氨酸氨基转移酶升高、血碱性磷酸酶升高和皮疹。','本品主要成份为氟康唑']\n",
    "\n",
    "model.match(source_sentences,target_sentences)\n",
    "model.get_matches()"
   ]
  },
  {
   "source": [
    "### 其他"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "method"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "type(model.get_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                From                      To  \\\n",
       "0  The main ingredient of this product is flucona...              本品主要成份为氟康唑   \n",
       "1  It allows continued monitoring of the benefit/...      它允许持续监测药品的收益/风险平衡。   \n",
       "2  Healthcare professionals are asked to report a...  要求医疗保健专业人员报告任何可疑的不良反应。   \n",
       "\n",
       "   Similarity  \n",
       "0       0.782  \n",
       "1       0.924  \n",
       "2       0.892  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>From</th>\n      <th>To</th>\n      <th>Similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The main ingredient of this product is flucona...</td>\n      <td>本品主要成份为氟康唑</td>\n      <td>0.782</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>It allows continued monitoring of the benefit/...</td>\n      <td>它允许持续监测药品的收益/风险平衡。</td>\n      <td>0.924</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Healthcare professionals are asked to report a...</td>\n      <td>要求医疗保健专业人员报告任何可疑的不良反应。</td>\n      <td>0.892</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "df=model.get_matches()\n",
    "df.loc[df['Similarity'] > 0.7] #filter df by column values\n",
    "df"
   ]
  },
  {
   "source": [
    "#### 下面这个读文本文件的函数不能用，读入之后不含换行符\\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_as_text(file_name, quotechar=None):\n",
    "    '''\n",
    "    input:txt\n",
    "    output:string\n",
    "    '''\n",
    "    import csv\n",
    "    import itertools\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quotechar=quotechar)\n",
    "        lines = []\n",
    "        for line in reader:\n",
    "            if sys.version_info[0] == 2:\n",
    "                line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "            lines.append(line)\n",
    "        content=''.join(list(itertools.chain(*lines)))\n",
    "    return content\n",
    "    # return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_passage = read_file_as_text('c:\\\\Users\\\\zhouy217\\\\OneDrive - Pfizer\\\\Documents\\\\data_pipeline\\\\data\\\\text2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"I enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is http://t.co/4ftYom0i. It's awesome you'll luv it #HadFun #Enjoyed BFN GNBest of all, NLTK is a free, open source, community-driven project.NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”\""
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "en_passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}