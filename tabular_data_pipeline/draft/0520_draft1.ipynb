{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.13 64-bit ('Simi_com': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ca1a6c70ba0ef033524d16c288d1083a318dfe37e6b1f3c85cb25e894a9c8d20"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TextData \n",
    "## Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 英文"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function unescape in module html:\n\nunescape(s)\n    Convert all named and numeric character references (e.g. &gt;, &#62;,\n    &x3e;) in the string s to the corresponding unicode characters.\n    This function uses the rules defined by the HTML 5 standard\n    for both valid and invalid character references, and the list of\n    HTML 5 named character references defined in html.entities.html5.\n\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "help(html.unescape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "type(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is  It's awesome you'll luv it HadFun Enjoyed BFN GN\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import itertools\n",
    "from autocorrect import Speller\n",
    "tweet=\"I enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is http://t.co/4ftYom0i It's awesome you'll luv it #HadFun #Enjoyed BFN GN\"\n",
    "\n",
    "def rm_nontext(text):\n",
    "    text_rmurl=html.unescape(text)\n",
    "    # remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.\\S+', \"\", text_rmurl)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    # remove old style retweet text \"RT\"\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    return text\n",
    "# def rm_nontext(text):\n",
    "#     text_rmurl=html.unescape(text)\n",
    "#     # remove hyperlinks\n",
    "#     text = re.sub(r'https?:\\/\\/.\\S+#^RT[\\s]+', \"\", text_rmurl)\n",
    "\n",
    "#     return text\n",
    "tweet_rm1=rm_nontext(tweet)\n",
    "print(tweet_rm1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i enjoyed the event which took place yesterday & i loved itt ! the link to the show is it is awesome you will luv it had fun enjoyed bin gn\n"
     ]
    }
   ],
   "source": [
    "def conv_text(text):\n",
    "    #缩写还原用完整全称替换#分开单词,比如forthewin#转换为小写#俚语转换#词形还原#拼写检查\n",
    "    Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n",
    "            \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "    for key,value in Apos_dict.items():\n",
    "        if key in text:\n",
    "            text=text.replace(key,value)        \n",
    " ###########################################################################            \n",
    "    text = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",text) if s])\n",
    "############################################################################\n",
    "    text=text.lower()\n",
    "############################################################################\n",
    "    file=open(\"./slang.txt\",\"r\")\n",
    "    slang=file.read()\n",
    "    slang=slang.split('\\n')\n",
    "    text_tokens=text.split()\n",
    "    slang_word=[]\n",
    "    meaning=[]\n",
    "\n",
    "    for line in slang:\n",
    "        temp=line.split(\"=\")\n",
    "        slang_word.append(temp[0])\n",
    "        meaning.append(temp[-1])\n",
    "\n",
    "    for i,word in enumerate(text_tokens):\n",
    "        if word in slang_word:\n",
    "            idx=slang_word.index(word)\n",
    "            text_tokens[i]=meaning[idx]\n",
    "            \n",
    "    text=\" \".join(text_tokens)\n",
    "#####################################################################################\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    spell = Speller(lang='en')\n",
    "    #spell check\n",
    "    text=spell(text)\n",
    "    return text\n",
    "tweet_rm2=conv_text(tweet_rm1)\n",
    "print(tweet_rm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After removing HTML characters the tweet is:-\nI enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is http://t.co/4ftYom0i It's awesome you'll luv it #HadFun #Enjoyed BFN GN\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "tweet=\"I enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is http://t.co/4ftYom0i It's awesome you'll luv it #HadFun #Enjoyed BFN GN\"\n",
    "\n",
    "tweet=html.unescape(tweet)\n",
    "\n",
    "print(\"After removing HTML characters the tweet is:-\\n{}\".format(tweet))\n",
    "#.format()用法：按照{}的顺序依次匹配括号中的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After removing Hashtags,URLs and Styles the tweet is:-\nI enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is  It's awesome you'll luv it HadFun Enjoyed BFN GN\n"
     ]
    }
   ],
   "source": [
    "#移除网址和话题标签\n",
    "#library for regular expressions\n",
    "import re\n",
    "\n",
    "# remove hyperlinks\n",
    "tweet = re.sub(r'https?:\\/\\/.\\S+', \"\", tweet)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "# remove old style retweet text \"RT\"\n",
    "tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "print(\"After removing Hashtags,URLs and Styles the tweet is:-\\n{}\".format(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After Contraction replacement the tweet is:-\nI enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is  It is awesome you will luv it HadFun Enjoyed BFN GN\n"
     ]
    }
   ],
   "source": [
    "#缩写还原\n",
    "#dictionary consisting of the contraction and the actual value\n",
    "Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n",
    "\t\t\"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "\n",
    "#replace the contractions\n",
    "for key,value in Apos_dict.items():\n",
    "\tif key in tweet:\n",
    "\t\ttweet=tweet.replace(key,value)\n",
    "\n",
    "print(\"After Contraction replacement the tweet is:-\\n{}\".format(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After splitting attached words the tweet is:-\nI enjoyd the event which took place yesteday & I lovdddd itttt !  The link to the show is   It is awesome you will luv it  Had Fun  Enjoyed  BFN GN\n"
     ]
    }
   ],
   "source": [
    "#分开单词\n",
    "import re\n",
    "#separate the words\n",
    "tweet = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",tweet) if s])\n",
    "print(\"After splitting attached words the tweet is:-\\n{}\".format(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After converting to lower case the tweet is:-\ni enjoyd the event which took place yesteday & i lovdddd itttt !  the link to the show is   it is awesome you will luv it  had fun  enjoyed  bfn gn\n"
     ]
    }
   ],
   "source": [
    "#转换为小写\n",
    "#convert to lower case\n",
    "tweet=tweet.lower()\n",
    "print(\"After converting to lower case the tweet is:-\\n{}\".format(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After slang replacement the tweet is:-\ni enjoyd the event which took place yesteday & i lovdddd itttt ! the link to the show is it is awesome you will luv it had fun enjoyed bfn gn\n"
     ]
    }
   ],
   "source": [
    "#俚语转换\n",
    "#open the file slang.txt\n",
    "file=open(\"./slang.txt\",\"r\")\n",
    "slang=file.read()\n",
    "\n",
    "#separating each line present in the file\n",
    "slang=slang.split('\\n')\n",
    "\n",
    "tweet_tokens=tweet.split()\n",
    "slang_word=[]\n",
    "meaning=[]\n",
    "\n",
    "#store the slang words and meanings in different lists\n",
    "for line in slang:\n",
    "\ttemp=line.split(\"=\")\n",
    "\tslang_word.append(temp[0])\n",
    "\tmeaning.append(temp[-1])\n",
    "\n",
    "#replace the slang word with meaning\n",
    "for i,word in enumerate(tweet_tokens):\n",
    "\tif word in slang_word:\n",
    "\t\tidx=slang_word.index(word)\n",
    "\t\ttweet_tokens[i]=meaning[idx]\n",
    "\t\t\n",
    "tweet=\" \".join(tweet_tokens)\n",
    "print(\"After slang replacement the tweet is:-\\n{}\".format(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "After standardizing the tweet is:-\ni enjoyd the event which took place yesteday & i lovdd itt ! the link to the show is it is awesome you will luv it had fun enjoyed bfn gn\nAfter Spell check the tweet is:-\ni enjoyed the event which took place yesterday & i loved itt ! the link to the show is it is awesome you will luv it had fun enjoyed bin gn\n"
     ]
    }
   ],
   "source": [
    "#词形还原\n",
    "import itertools\n",
    "#One letter in a word should not be present more than twice in continuation\n",
    "tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "print(\"After standardizing the tweet is:-\\n{}\".format(tweet))\n",
    "\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "#spell check\n",
    "tweet=spell(tweet)\n",
    "print(\"After Spell check the tweet is:-\\n{}\".format(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhouy217\\AppData\\Roaming\\nltk_data...\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhouy217\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    " \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x9d in position 183: illegal multibyte sequence",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1\\ipykernel_19476\\2215837411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Use this to read file content as a stream:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x9d in position 183: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(\"text.txt\")\n",
    " \n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "source": [
    "### 中文\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on built-in function strip:\n\nstrip(chars=None, /) method of builtins.str instance\n    Return a copy of the string with leading and trailing whitespace removed.\n    \n    If chars is given and not None, remove characters in chars instead.\n\n"
     ]
    }
   ],
   "source": [
    "help(line.strip)#返回删除前后副本被移除了的字符串副本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function compile in module re:\n\ncompile(pattern, flags=0)\n    Compile a regular expression pattern, returning a Pattern object.\n\n"
     ]
    }
   ],
   "source": [
    "help(re.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sub' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1\\ipykernel_19476\\1068842197.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sub' is not defined"
     ]
    }
   ],
   "source": [
    "help(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for data clean.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for read data for data clean\"\"\"\n",
    "    def get_examples(self, input_file):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def write_examples(self, examples, output_file):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def _read_file(cls, input_file, quotechar=None):\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t', quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "class DemoProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the Demo data set.\"\"\"\n",
    "\n",
    "    def get_examples(self, input_file):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {}\".format(input_file))\n",
    "        return self._create_examples(\n",
    "            self._read_file(input_file), \"data\")\n",
    "\n",
    "    def write_examples(self, examples, output_file):\n",
    "        logger.info(\"WRITING TO OUTPUT FILE {}\".format(output_file))\n",
    "        with open(output_file, 'w', encoding='utf-8') as writer:\n",
    "            for example in examples:\n",
    "                writer.write(example.label + '\\t' + example.text_a + '\\n')\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[1]\n",
    "            text_b = None\n",
    "            label = line[0]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "#  去URL\n",
    "def rmURL(line):\n",
    "    import re\n",
    "    pattern = re.compile(r'http[a-zA-Z0-9.?/&=:]*')\n",
    "    return pattern.sub('', line.strip())\n",
    "\n",
    "\n",
    "# 去非中文、英文、数字字符， 如表情，小语种字符\n",
    "def rmUNK(line):\n",
    "    import re\n",
    "    pattern = re.compile(u'[^0-9a-zA-Z\\u4e00-\\u9fa5.，,。？“”]+', re.UNICODE)\n",
    "    return pattern.sub('', line.strip())\n",
    "\n",
    "\n",
    "\n",
    "# 去停用词\n",
    "def rmStopwords(line, stopwords):\n",
    "    out = []\n",
    "    for word in line:\n",
    "        if word not in stopwords:\n",
    "            out.append(word)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument('--input_file',\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='The input file which need clean.')\n",
    "\n",
    "    parser.add_argument('--output_file',\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='The output file of cleaned data')\n",
    "\n",
    "    parser.add_argument('--task_name',\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=True,\n",
    "        choices=['demo'],\n",
    "        help='The name of the task to clean.')\n",
    "\n",
    "\n",
    "    # Optional parameters\n",
    "    parser.add_argument('--rm_url',\n",
    "        default=True,\n",
    "        type=bool,\n",
    "        help='whether to remove url link.')\n",
    "\n",
    "    parser.add_argument('--rm_unknown_char',\n",
    "        default=True,\n",
    "        type=bool,\n",
    "        help='whether to remove unknown char')\n",
    "\n",
    "    parser.add_argument('--jieba_cut',\n",
    "        default=True,\n",
    "        type=bool,\n",
    "        help='whether to use jieba to cut the sentence.')\n",
    "\n",
    "    parser.add_argument('--jieba_vocab_file',\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help='Custom vocab file for jieba.')\n",
    "\n",
    "    parser.add_argument('--stopwords_file',\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help='whether to remove the stopwords and stopwords vocab file.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    processors = {\n",
    "        'demo': DemoProcessor,\n",
    "        # add in here your processor, key: should be lower case\n",
    "    }\n",
    "\n",
    "    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                        level = logging.INFO)\n",
    "\n",
    "    if not os.path.exists(args.input_file):\n",
    "        raise ValueError('input_file {} does not exist, should be a data file which need to be cleaned.'.format(args.input_file))\n",
    "    if os.path.exists(args.output_file):\n",
    "        raise ValueError('input_file {} has existed, should be a new file.'.format(args.output_file))\n",
    "\n",
    "    task_name = args.task_name.lower()\n",
    "    if task_name not in processors:\n",
    "        raise ValueError('Task nor found: %s' % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "    data_examples = processor.get_examples(args.input_file)\n",
    "    # data_examples  [inputexample1, inputexample2, inputexample3,  ...]\n",
    "\n",
    "    if args.rm_url:\n",
    "        logging.info('starting remove url.')\n",
    "        url_examples = data_examples\n",
    "        data_examples = []\n",
    "        for url_example in url_examples:\n",
    "            guid = url_example.guid\n",
    "            text_a = url_example.text_a\n",
    "            text_b = url_example.text_b\n",
    "            label = url_example.label\n",
    "\n",
    "            text_a = rmURL(text_a)\n",
    "            if text_b is not None:\n",
    "                text_b = rmURL(text_b)\n",
    "            example = InputExample(guid, text_a, text_b, label)\n",
    "            data_examples.append(example)\n",
    "        logging.info('finishing remove url.')\n",
    "\n",
    "\n",
    "    if args.rm_unknown_char:\n",
    "        logging.info('starting remove unknown char.')\n",
    "        unknown_char_examples = data_examples\n",
    "        data_examples = []\n",
    "        for unknown_char_example in unknown_char_examples:\n",
    "            guid = unknown_char_example.guid\n",
    "            text_a = unknown_char_example.text_a\n",
    "            text_b = unknown_char_example.text_b\n",
    "            label = unknown_char_example.label\n",
    "\n",
    "            text_a = rmUNK(text_a)\n",
    "            if text_b is not None:\n",
    "                text_b = rmUNK(text_b)\n",
    "            example = InputExample(guid, text_a, text_b, label)\n",
    "            data_examples.append(example)\n",
    "\n",
    "        logging.info('finishing remove unknown char.')\n",
    "\n",
    "\n",
    "    if args.jieba_cut:\n",
    "        logging.info('starting tokenization.')\n",
    "        import jieba\n",
    "        if args.jieba_vocab_file:\n",
    "            jieba.load_userdict(args.jieba_vocab_file)\n",
    "\n",
    "        jieba_examples = data_examples\n",
    "        data_examples = []\n",
    "        for jieba_example in jieba_examples:\n",
    "            guid = jieba_example.guid\n",
    "            text_a = jieba_example.text_a\n",
    "            text_b = jieba_example.text_b\n",
    "            label = jieba_example.label\n",
    "\n",
    "            text_a = list(jieba.cut(text_a.strip()))\n",
    "            if text_b is not None:\n",
    "                text_b = list(jieba.cut(text_b.strip()))\n",
    "            example = InputExample(guid, text_a, text_b, label)\n",
    "            data_examples.append(example)\n",
    "        logging.info('finishing tokenization.')\n",
    "\n",
    "\n",
    "    if args.stopwords_file:\n",
    "        logging.info('starting remove stopwords.')\n",
    "        stopwords = []\n",
    "        with open(args.stopwords_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                stopwords.append(line.strip())\n",
    "\n",
    "        stopwords_examples = data_examples\n",
    "        data_examples = []\n",
    "        for stopwords_example in stopwords_examples:\n",
    "            guid = stopwords_example.guid\n",
    "            text_a = stopwords_example.text_a\n",
    "            text_b = stopwords_example.text_b\n",
    "            label = stopwords_example.label\n",
    "\n",
    "            text_a = rmStopwords(text_a, stopwords)\n",
    "            if text_b is not None:\n",
    "                text_b = rmStopwords(text_b, stopwords)\n",
    "            example = InputExample(guid, ' '.join(text_a), text_b, label)\n",
    "            data_examples.append(example)\n",
    "        logging.info('finishing remove stopwords.')\n",
    "\n",
    "    # 做一些其他清理工作\n",
    "    logging.info('starting other clean operation.')\n",
    "    otherOP_examples = data_examples\n",
    "    data_examples = []\n",
    "    for otherOP_example in otherOP_examples:\n",
    "        guid = otherOP_example.guid\n",
    "        text_a = otherOP_example.text_a\n",
    "        text_b = otherOP_example.text_b\n",
    "        label = otherOP_example.label\n",
    "\n",
    "        # text_a = text_a.replace('\\n', '').replace('\\r', '')\n",
    "        if text_b is not None:\n",
    "            pass\n",
    "            # text_b = text_b.replace('\\n', '').replace('\\r', '')\n",
    "        example = InputExample(guid, text_a, text_b, label)\n",
    "        data_examples.append(example)\n",
    "\n",
    "    logging.info('finishing other clean operation.')\n",
    "\n",
    "\n",
    "    processor.write_examples(data_examples, args.output_file)\n",
    "    logging.info('Data clean finished!')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ]
}