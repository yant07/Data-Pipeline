{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.13 64-bit ('Simi_com': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ca1a6c70ba0ef033524d16c288d1083a318dfe37e6b1f3c85cb25e894a9c8d20"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TextData \n",
    "## Data Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 预处理部分"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "from html.parser import HTMLParser\n",
    "import string\n",
    "import re\n",
    "import itertools\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import jieba\n",
    "import re\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入文件\n",
    "import csv\n",
    "import itertools\n",
    "def _read_file(input_file, quotechar=None):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quotechar=quotechar)\n",
    "        lines = []\n",
    "        for line in reader:\n",
    "            if sys.version_info[0] == 2:\n",
    "                line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "            lines.append(line)\n",
    "        content=''.join(list(itertools.chain(*lines)))\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'乳腺癌是乳腺上皮细胞在多种致癌因子的作用下，发生增殖失控的现象。疾病早期常表现为乳房肿块、乳头溢液、腋窝淋巴结肿大等症状，晚期可因癌细胞发生远处转移，出现多器官病变，直接威胁患者的生命。乳腺癌常被称为“粉红杀手”，其发病率位居女性恶性肿瘤的首位，男性乳腺癌较为少见。随着医疗水平的提高，乳腺癌已成为疗效最佳的实体肿瘤之一。宫颈癌和乳腺癌并称女性两大“隐性杀手”。结核病是由结核分枝杆菌引起的慢性传染病，可侵及许多脏器，以肺部结核感染最为常见。排菌者为其重要的传染源。人体感染结核菌后不一定发病，当抵抗力降低或细胞介导的变态反应增高时，才可能引起临床发病。若能及时诊断，并予合理治疗，大多可获临床痊愈。'"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "a=_read_file('Chinese_text.txt')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"I enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is http://t.co/4ftYom0i It's awesome you'll luv it #HadFun #Enjoyed BFN GNBest of all, NLTK is a free, open source, community-driven project.NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”\""
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "b=_read_file('text2.txt')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Python is a high-level, interpreted, interactive and object-oriented scripting language. Python is designed to be highly readable. It uses English keywords frequently where as other languages use punctuation, and it has fewer syntactical constructions than other languages.Python was developed by Guido van Rossum in the late eighties and early nineties at the National Research Institute for Mathematics and Computer Science in the Netherlands.Python is derived from many other languages, including ABC, Modula-3, C, C++, Algol-68, SmallTalk, and Unix shell and other scripting languages.Python is copyrighted. Like Perl, Python source code is now available under the GNU General Public License (GPL).Python is now maintained by a core development team at the institute, although Guido van Rossum still holds a vital role in directing its progress.Another input is the mask image (cloud.png). The final Result is on the right side.The experience was bad as hell.This app is really helpful.Damn the app tastes like shit.Please don't download the app you will regret it.I love the app is amazing .\""
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "e=_read_file('text3.txt')\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去掉网址，#等非英文字符的内容\n",
    "def rm_nontext(text):\n",
    "    text_rmurl=html.unescape(text)\n",
    "    text = re.sub(r'https?:\\/\\/.\\S+', \"\", text_rmurl)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    return text\n",
    " #缩写还原,用完整全称替换;分开单词,比如forthewin;统一转换为小写；俚语转换；词形还原；拼写检查\n",
    "def conv_text(text):\n",
    "    Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n",
    "            \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "    for key,value in Apos_dict.items():\n",
    "        if key in text:\n",
    "            text=text.replace(key,value)        \n",
    "        \n",
    "    text = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",text) if s])\n",
    "\n",
    "    text=text.lower()\n",
    "\n",
    "    file=open(\"./slang.txt\",\"r\")\n",
    "    slang=file.read()\n",
    "    slang=slang.split('\\n')\n",
    "    text_tokens=text.split()\n",
    "    slang_word=[]\n",
    "    meaning=[]\n",
    "\n",
    "    for line in slang:\n",
    "        temp=line.split(\"=\")\n",
    "        slang_word.append(temp[0])\n",
    "        meaning.append(temp[-1])\n",
    "\n",
    "    for i,word in enumerate(text_tokens):\n",
    "        if word in slang_word:\n",
    "            idx=slang_word.index(word)\n",
    "            text_tokens[i]=meaning[idx]\n",
    "            \n",
    "    text=\" \".join(text_tokens)\n",
    "\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    spell = Speller(lang='en')\n",
    "    #spell check\n",
    "    text=spell(text)\n",
    "    return text\n",
    "#去停用词\n",
    "def rm_stopwords(text):\n",
    "    stopwords_eng = stopwords.words('english')\n",
    "    \n",
    "    text_tokens=text.split()\n",
    "    text_list=[]\n",
    "    #remove stopwords\n",
    "    for word in text_tokens:\n",
    "        if word not in stopwords_eng:\n",
    "            text_list.append(word)\n",
    "    return text_list\n",
    "#去标点\n",
    "def rm_punc(text_list):\n",
    "    clean_text=[]\n",
    "    #remove punctuations\n",
    "    for word in text_list:\n",
    "        if word not in string.punctuation:\n",
    "            clean_text.append(word)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['python', 'high-level,', 'interpreted,', 'interactive', 'object-oriented', 'scripting', 'language.', 'python', 'designed', 'highly', 'readable.', 'uses', 'english', 'keywords', 'frequently', 'languages', 'use', 'punctuation,', 'fewer', 'syntactic', 'constructions', 'languages.', 'python', 'developed', 'guide', 'van', 'possum', 'late', 'eighties', 'early', 'nineties', 'national', 'research', 'institute', 'mathematics', 'computer', 'science', 'netherlands.', 'python', 'derived', 'many', 'languages,', 'including', 'abc,', 'module-3,', 'c,', 'c++,', 'algal-68,', 'small', 'talk,', 'unix', 'shell', 'scripting', 'languages.', 'python', 'copyrighted.', 'like', 'perl,', 'python', 'source', 'code', 'available', 'gnu', 'general', 'public', 'license', 'gpl).', 'python', 'maintained', 'core', 'development', 'team', 'institute,', 'although', 'guide', 'van', 'possum', 'still', 'holds', 'vital', 'role', 'directing', 'progress.', 'another', 'input', 'mask', 'image', '(cloud.png).', 'final', 'result', 'right', 'side.']\n"
     ]
    }
   ],
   "source": [
    "tweet_rm1=rm_nontext(e)\n",
    "tweet_rm2=conv_text(tweet_rm1)\n",
    "# print(tweet_rm2)\n",
    "textls=rm_stopwords(tweet_rm2)\n",
    "# print(textls)\n",
    "en_text=rm_punc(textls)\n",
    "print(en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':  # 判断一个uchar是否是汉字\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    " \n",
    "def allcontents(contents):\n",
    "    content = ''\n",
    "    for i in contents:\n",
    "        if is_chinese(i):\n",
    "            content = content+i\n",
    "    # print('\\n处理后的句子为:\\n'+content)\n",
    "#先去停用词，再分词\n",
    "# 停用词表\n",
    "def get_stopword():\n",
    "    s=set()\n",
    "    with open('StopwordsList.txt','r',encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            s.add(line.strip())\n",
    "    return s\n",
    "\n",
    "# 去掉文本中的停用词\n",
    "def drop_stopwords(words):\n",
    "    content_dropped=[word for word in words if word not in stopword]\n",
    "    content=''.join(list(itertools.chain(*content_dropped)))\n",
    "    return content\n",
    "# 对文本进行jieba分词\n",
    "def fenci(datas):\n",
    "    # cut_words = map(lambda s: list(jieba.cut(s)), datas)\n",
    "    cut_words=jieba.cut(datas, cut_all=False, HMM=True)\n",
    "    fenci_list=list(cut_words)\n",
    "    print(\"总词汇量：\"+str(len(fenci_list)))\n",
    "    return fenci_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "总词汇量：99\n['乳腺癌', '乳腺', '上皮细胞', '致癌', '子作', '下', '发生', '增殖', '失控', '现象', '疾病', '早期', '常', '表现', '乳房', '肿块', '乳头', '溢液', '腋窝', '淋巴结', '肿大', '症状', '晚期', '癌细胞', '发生', '远处', '转移', '出现', '器官', '病变', '直接', '威胁', '患', '生命', '乳腺癌', '常称', '粉红', '杀手', '发病率', '位居', '女性', '恶性肿瘤', '首位', '男性', '乳腺癌', '少见', '医疗', '水平', '提高', '乳腺癌', '已成', '疗效', '最佳', '实体肿瘤', '宫颈癌', '乳腺癌', '称', '女性', '两大', '隐性', '杀手', '结核病', '结核', '分枝杆菌', '引', '慢性', '传染病', '侵许', '脏器', '肺部', '结核', '感染', '最', '常见', '排菌重', '传染源', '人体', '感染', '结核菌', '后', '不定', '发病', '抵抗力', '降低', '细胞', '介导', '变态反应', '增高', '时才', '引床', '发病', '时', '诊断', '予', '合理', '治疗', '大', '获床', '痊愈']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "centents = '1,2,3...how ara you *_*开始吧， 加油！乳腺癌是乳腺上皮细胞在多种致癌因子的作用下，发生增殖失控的现象。疾病早期常表现为乳房肿块、乳头溢液、腋窝淋巴结肿大等症状，晚期可因癌细胞发生远处转移，出现多器官病变，直接威胁患者的生命。'\n",
    "# print('原句子为:\\n'+centents)\n",
    "sentence = allcontents(centents)\n",
    "\n",
    "stopword=get_stopword()\n",
    "contents=drop_stopwords(a)\n",
    "contents\n",
    "fenci_con=fenci(contents)\n",
    "print(fenci_con)"
   ]
  },
  {
   "source": [
    "### sentiment analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#英文\n",
    "text=e\n",
    "# create a TextBlob object\n",
    "blob_object = TextBlob(text)\n",
    "text_sentence=blob_object.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Sentence(\"Python is a high-level, interpreted, interactive and object-oriented scripting language.\"),\n",
       " Sentence(\"Python is designed to be highly readable.\"),\n",
       " Sentence(\"It uses English keywords frequently where as other languages use punctuation, and it has fewer syntactical constructions than other languages.Python was developed by Guido van Rossum in the late eighties and early nineties at the National Research Institute for Mathematics and Computer Science in the Netherlands.Python is derived from many other languages, including ABC, Modula-3, C, C++, Algol-68, SmallTalk, and Unix shell and other scripting languages.Python is copyrighted.\"),\n",
       " Sentence(\"Like Perl, Python source code is now available under the GNU General Public License (GPL).Python is now maintained by a core development team at the institute, although Guido van Rossum still holds a vital role in directing its progress.Another input is the mask image (cloud.png).\"),\n",
       " Sentence(\"The final Result is on the right side.The experience was bad as hell.This app is really helpful.Damn the app tastes like shit.Please don't download the app you will regret it.I love the app is amazing .\")]"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "text_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Python is a high-level, interpreted, interactive and object-oriented scripting language.', 'Python is designed to be highly readable.', 'It uses English keywords frequently where as other languages use punctuation, and it has fewer syntactical constructions than other languages.', 'Python was developed by Guido van Rossum in the late eighties and early nineties at the National Research Institute for Mathematics and Computer Science in the Netherlands.', 'Python is derived from many other languages, including ABC, Modula-3, C, C++, Algol-68, SmallTalk, and Unix shell and other scripting languages.', 'Python is copyrighted.', 'Like Perl, Python source code is now available under the GNU General Public License (GPL).Python is now maintained by a core development team at the institute, although Guido van Rossum still holds a vital role in directing its progress.', 'Another input is the mask image (cloud.png).', 'The final Result is on the right side.', 'The experience was bad as hell.', 'This app is really helpful.', 'Damn the app tastes like shit.', \"Please don't download the app you will regret it.\", 'I love the app is amazing .']\n"
     ]
    }
   ],
   "source": [
    "def split_sentence(text_str):\n",
    "    sen_list=[]\n",
    "    nlp = spacy.load(\"en_core_web_sm\") \n",
    "    doc = nlp(text_str)   \n",
    "    for sent in doc.sents:\n",
    "        s=str(sent)\n",
    "        sen_list.append(s)\n",
    "    return sen_list\n",
    "\n",
    "split_text=split_sentence(e)\n",
    "print(split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_ana(sentence_list):\n",
    "    feedbacks = sentence_list\n",
    "    positive_feedbacks = []\n",
    "    negative_feedbacks = []\n",
    "\n",
    "    for feedback in feedbacks:\n",
    "        feedback_polarity = TextBlob(feedback).sentiment.polarity\n",
    "        if feedback_polarity > 0:\n",
    "            positive_feedbacks.append(feedback)\n",
    "            continue\n",
    "        negative_feedbacks.append(feedback)\n",
    "\n",
    "    print('Positive_feebacks Count : {}'.format(len(positive_feedbacks)))\n",
    "    print(positive_feedbacks)\n",
    "    print('Negative_feedback Count : {}'.format(len(negative_feedbacks)))\n",
    "    print(negative_feedbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Positive_feebacks Count : 6\n['Python is designed to be highly readable.', 'Python is derived from many other languages, including ABC, Modula-3, C, C++, Algol-68, SmallTalk, and Unix shell and other scripting languages.', 'Like Perl, Python source code is now available under the GNU General Public License (GPL).Python is now maintained by a core development team at the institute, although Guido van Rossum still holds a vital role in directing its progress.', 'The final Result is on the right side.', 'This app is really helpful.', 'I love the app is amazing .']\nNegative_feedback Count : 8\n['Python is a high-level, interpreted, interactive and object-oriented scripting language.', 'It uses English keywords frequently where as other languages use punctuation, and it has fewer syntactical constructions than other languages.', 'Python was developed by Guido van Rossum in the late eighties and early nineties at the National Research Institute for Mathematics and Computer Science in the Netherlands.', 'Python is copyrighted.', 'Another input is the mask image (cloud.png).', 'The experience was bad as hell.', 'Damn the app tastes like shit.', \"Please don't download the app you will regret it.\"]\n"
     ]
    }
   ],
   "source": [
    "sentiment_ana(split_text)"
   ]
  },
  {
   "source": [
    "### 文本相似度"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y):\n",
    "  \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
    "  intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "  union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "  return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "sentences = [\"The bottle is empty\",\n",
    "\"There is nothing in the bottle\"]\n",
    "sentences_jac = [sent.lower().split(\" \") for sent in sentences]\n",
    "jaccard_similarity(sentences_jac[0], sentences_jac[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, pow, exp\n",
    " \n",
    "def squared_sum(x):\n",
    "  \"\"\" return 3 rounded square rooted value \"\"\"\n",
    "  return round(sqrt(sum([a*a for a in x])),3)\n",
    " \n",
    "def euclidean_distance(x,y):\n",
    "  \"\"\" return euclidean distance between two lists \"\"\"\n",
    "  return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    "\n",
    "def distance_to_similarity(distance):\n",
    "  return 1/(exp(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.7483274802858235\n运用欧氏距离计算的相似度0.17406482698308973\n"
     ]
    }
   ],
   "source": [
    "embeddings = [nlp(sentence).vector for sentence in sentences]\n",
    "distance = euclidean_distance(embeddings[0], embeddings[1])\n",
    "print(distance)#计算出距离后需要转换成相似度\n",
    "eu_similarity=distance_to_similarity(distance) \n",
    "print('运用欧氏距离计算的相似度'+str(eu_similarity))\n",
    "#欧式距离计算相似度再检查一下"
   ]
  },
  {
   "source": [
    "### LDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fenci_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import codecs\n",
    "def lda_model(fenci_con):\n",
    "    train = []\n",
    "    for w in fenci_con:\n",
    "        train.append([w])\n",
    "    dictionary = corpora.Dictionary(train)\n",
    "    corpus = [dictionary.doc2bow(text) for text in train]\n",
    "    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=60)\n",
    "    # num_topics：主题数目\n",
    "    # passes：训练伦次\n",
    "    # num_words：每个主题下输出的term的数目\n",
    "\n",
    "    for topic in lda.print_topics(num_words = 10):\n",
    "        termNumber = topic[0]\n",
    "        print(topic[0], ':', sep='')\n",
    "        listOfTerms = topic[1].split('+')\n",
    "        for term in listOfTerms:\n",
    "            listItems = term.split('*')\n",
    "            print('  ', listItems[1], '(', listItems[0], ')', sep='')\n",
    "\n",
    "    print('\\nPerplexity: ', lda.log_perplexity(corpus))#The LDA model (lda_model) we have created above can be used to compute the model’s perplexity, i.e. how good the model is. The lower the score the better the model will be.\n",
    "    import pyLDAvis.gensim_models\n",
    "    d=pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)   \n",
    "    pyLDAvis.save_html(d, 'lda_result.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\n  \"python\" (0.092)\n  \"possum\" ( 0.031)\n  \"guide\" ( 0.031)\n  \"language.\" ( 0.018)\n  \"keywords\" ( 0.018)\n  \"mathematics\" ( 0.018)\n  \"c,\" ( 0.018)\n  \"input\" ( 0.018)\n  \"punctuation,\" ( 0.018)\n  \"late\"( 0.018)\n1:\n  \"languages.\" (0.027)\n  \"scripting\" ( 0.027)\n  \"van\" ( 0.027)\n  \"still\" ( 0.016)\n  \"interpreted,\" ( 0.016)\n  \"abc,\" ( 0.016)\n  \"including\" ( 0.016)\n  \"netherlands.\" ( 0.016)\n  \"vital\" ( 0.016)\n  \"module-3,\"( 0.016)\n\nPerplexity:  -5.303836750801632\n"
     ]
    }
   ],
   "source": [
    "lda_model(en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\n  \"乳腺癌\" (0.056)\n  \"感染\" ( 0.025)\n  \"结核\" ( 0.025)\n  \"威胁\" ( 0.015)\n  \"引床\" ( 0.015)\n  \"分枝杆菌\" ( 0.015)\n  \"已成\" ( 0.015)\n  \"介导\" ( 0.015)\n  \"抵抗力\" ( 0.015)\n  \"引\"( 0.015)\n1:\n  \"女性\" (0.027)\n  \"杀手\" ( 0.027)\n  \"发生\" ( 0.027)\n  \"发病\" ( 0.027)\n  \"男性\" ( 0.016)\n  \"称\" ( 0.016)\n  \"肿块\" ( 0.016)\n  \"增殖\" ( 0.016)\n  \"子作\" ( 0.016)\n  \"降低\"( 0.016)\n\nPerplexity:  -5.441214442091983\n"
     ]
    }
   ],
   "source": [
    "lda_model(fenci_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}