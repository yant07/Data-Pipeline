{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-06-10T10:52:48.927203Z","iopub.execute_input":"2022-06-10T10:52:48.927816Z","iopub.status.idle":"2022-06-10T10:52:48.959443Z","shell.execute_reply.started":"2022-06-10T10:52:48.927690Z","shell.execute_reply":"2022-06-10T10:52:48.958190Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:54:27.429676Z","iopub.execute_input":"2022-06-10T10:54:27.430616Z","iopub.status.idle":"2022-06-10T10:56:57.549746Z","shell.execute_reply.started":"2022-06-10T10:54:27.430559Z","shell.execute_reply":"2022-06-10T10:56:57.548613Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install nltk","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install \"https://github.com/megagonlabs/ginza/releases/download/latest/ginza-latest.tar.gz\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install spacy==2.3.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U ginza ja_ginza","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport datetime\nimport codecs\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pkg_resources, imp\nimp.reload(pkg_resources)\nimport numpy as np\nimport pandas as pd\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nfrom matplotlib import pyplot as plt\n# import neologdn\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport spacy\nfrom spacy.lang.ja import Japanese\nimport regex\nfrom wordcloud import WordCloud\nimport collections\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n# import shap\n# shap.initjs()\n# import oseti","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:53:05.175243Z","iopub.execute_input":"2022-06-10T10:53:05.176112Z","iopub.status.idle":"2022-06-10T10:53:42.258714Z","shell.execute_reply.started":"2022-06-10T10:53:05.176063Z","shell.execute_reply":"2022-06-10T10:53:42.257454Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nos.getcwd()\nprint(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:53:47.124874Z","iopub.execute_input":"2022-06-10T10:53:47.126100Z","iopub.status.idle":"2022-06-10T10:53:47.131768Z","shell.execute_reply.started":"2022-06-10T10:53:47.126051Z","shell.execute_reply":"2022-06-10T10:53:47.130806Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import re\nimport itertools\nimport unicodedata\nfrom janome.tokenizer import Tokenizer\nimport networkx as nx\nfrom scipy.spatial import distance\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'Droid Sans Japanese'","metadata":{"_cell_guid":"1b920f2f-f867-47cd-9c2d-9fb62453a282","_uuid":"5dc822122d1ede071209b1515c7cf2a3e81600b0","execution":{"iopub.status.busy":"2022-06-10T10:53:50.157161Z","iopub.execute_input":"2022-06-10T10:53:50.158328Z","iopub.status.idle":"2022-06-10T10:53:50.547426Z","shell.execute_reply.started":"2022-06-10T10:53:50.158278Z","shell.execute_reply":"2022-06-10T10:53:50.546351Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset into a pandas dataframe.\n# df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n# list=[10,11,15,16]\ncol_list=[15]\ndf = pd.read_excel(\"/kaggle/input/ja-nlp/ja_slide9_data.xlsx\", usecols=col_list,names=['sentence'])\n# Report the number of sentences.\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n\n# Display 10 random rows from the data.\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:53:58.137984Z","iopub.execute_input":"2022-06-10T10:53:58.138621Z","iopub.status.idle":"2022-06-10T10:53:58.195892Z","shell.execute_reply.started":"2022-06-10T10:53:58.138577Z","shell.execute_reply":"2022-06-10T10:53:58.194224Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tweets = df['sentence'].as_matrix()\n\nregex = u'[^\\u3041-\\u3093\\u30A1-\\u30F4\\u4E00-\\u9FCB]'\nt = Tokenizer()\n\ntweet_words = []\nfor tweet in tweets:\n    tweet = re.sub(regex, ' ', tweet)\n    words = []\n    for token in t.tokenize(tweet):\n        speechs = token.part_of_speech.split(',')\n        if ('名詞' in speechs) or ('形容詞' in speechs):\n            if len(token.surface) > 1:\n                words.append(token.surface)\n    tweet_words.append(words)\n\nlen(tweet_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_cnt = {}\nfor words in tweet_words:\n    for word in words:\n        if word not in word_cnt:\n            word_cnt[word] = 1\n        else:\n            word_cnt[word] += 1\n    \nword_cnt_df = pd.DataFrame({'word': [k for k in word_cnt.keys()], 'cnt': [v for v in word_cnt.values()]})\nlen(word_cnt_df)","metadata":{"_cell_guid":"591b02ca-1f73-40f1-9037-9bba85b9e6f0","_uuid":"b6e5986a63a39e80a195edd0881d4e06ddc2f6c4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_cnt_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_cnt_df[['cnt']].describe()","metadata":{"_cell_guid":"d5f7476a-b59f-43ab-97eb-dca451c92fb4","_uuid":"fda37cd240d5ac480311a132a5bd7f827cd5deae","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = word_cnt_df[word_cnt_df['cnt'] > 3]\ntmp.sort_values(by='cnt', ascending=False).plot(kind='bar', x='word', y='cnt', figsize=(15,7), legend=False)\nplt.show()","metadata":{"_cell_guid":"230b6db0-16e4-4afe-b60c-f7ee17dea1fa","_uuid":"ff2a290ba45e31a5e1b2f038b92e064e2754b4b8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 两个词的","metadata":{}},{"cell_type":"code","source":"def ja_preprocess(text):\n    nlp = spacy.load('ja_ginza')\n    lowered = text.lower()\n    normalized = neologdn.normalize(lowered)\n    tokenized = []\n    for i in nlp(normalized):\n        word = i.lemma_\n        pos = i.pos_\n        if pos in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\", \"PROPN\"] and len(word) > 1:\n#         and i.text not in stopwords_ja:\n            tokenized.append(word)\n    preprocessed = \" \".join(tokenized)\n    return preprocessed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preprocessed['ja_preprocessed'] = df['sentence'].apply(lambda x: ja_preprocess(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ngram_func(ngram, text_series):\n    string_filterd =  text_series.sum().split()\n    dic = nltk.FreqDist(nltk.ngrams(string_filterd, ngram)).most_common(30)\n    ngram_df = pd.DataFrame(dic, columns=['ngram','count'])\n    ngram_df.index = [' '.join(i) for i in ngram_df.ngram]\n    ngram_df.drop('ngram',axis=1, inplace=True)\n    return ngram_df\nuni_ja = hv.Bars(ngram_func(1, df_preprocessed['ja_preprocessed'])[::-1]).opts(title=\"Japanese Unigram Count top-30\", color=\"red\", xlabel=\"Unigrams\", ylabel=\"Count\")\nbi_ja = hv.Bars(ngram_func(2, df_preprocessed['ja_preprocessed'])[::-1]).opts(title=\"Japanese Bigram Count top-30\", color=\"blue\", xlabel=\"Bigrams\", ylabel=\"Count\")\n(uni_ja + bi_ja).opts(opts.Bars(width=380, height=600,tools=['hover'],show_grid=True,invert_axes=True,fontsize={'title':10})).opts(shared_axes=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tweet_combinations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = {}\ntarget_words = word_cnt_df[word_cnt_df['cnt'] > 3]['word'].as_matrix()\nfor word in target_words:\n    if word not in vocab:\n        vocab[word] = len(vocab)\n\nre_vocab = {}\nfor word, i in vocab.items():\n    re_vocab[i] = word\n    \nlen(vocab)","metadata":{"_cell_guid":"2958b3be-85ac-4c55-952e-9962a61c5fce","_uuid":"238b866a6b9a6498690c97aa949e06b0c53c3ec0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_combinations = [list(itertools.combinations(words, 2)) for words in tweet_words]\ncombination_matrix = np.zeros((len(vocab), len(vocab)))\n\nfor tweet_comb in tweet_combinations:\n    for comb in tweet_comb:\n        if comb[0] in target_words and comb[1] in target_words:\n            combination_matrix[vocab[comb[0]], vocab[comb[1]]] += 1\n            combination_matrix[vocab[comb[1]], vocab[comb[0]]] += 1\n        \nfor i in range(len(vocab)):\n    combination_matrix[i, i] /= 2\n        \ncombination_matrix","metadata":{"_cell_guid":"15409725-d66a-484b-9483-b9b39bd57012","_uuid":"5aa2ece0c6885467aa8c757c4fffd56b4c115557","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaccard_matrix = 1 - distance.cdist(combination_matrix, combination_matrix, 'jaccard')\njaccard_matrix","metadata":{"_cell_guid":"f87d4282-4225-437c-b8f2-9afdd4a97150","_uuid":"104df2f45923389621f48955c122d0fc5ee62b83","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nodes = []\n\nfor i in range(len(vocab)):\n    for j in range(i+1, len(vocab)):\n        jaccard = jaccard_matrix[i, j]\n        if jaccard > 0:\n            nodes.append([re_vocab[i], re_vocab[j], word_cnt[re_vocab[i]], word_cnt[re_vocab[j]], jaccard])\n            \nlen(nodes)","metadata":{"_cell_guid":"40faf21f-d07a-406b-8c05-8166eab87073","_uuid":"bc11eff5f7a5dbc7e60f499d4286d66c65a810f5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"G = nx.Graph()\nG.nodes(data=True)\n\nfor pair in nodes:\n    node_x, node_y, node_x_cnt, node_y_cnt, jaccard = pair[0], pair[1], pair[2], pair[3], pair[4]\n    if not G.has_node(node_x):\n        G.add_node(node_x, count=node_x_cnt)\n    if not G.has_node(node_y):\n        G.add_node(node_y, count=node_y_cnt)\n    if not G.has_edge(node_x, node_y):\n        G.add_edge(node_x, node_y, weight=jaccard)\n        \nplt.figure(figsize=(15,15))\npos = nx.spring_layout(G, k=0.1)\n\nnode_size = [d['count']*100 for (n,d) in G.nodes(data=True)]\nnx.draw_networkx_nodes(G, pos, node_color='cyan', alpha=1.0, node_size=node_size)\nnx.draw_networkx_labels(G, pos, fontsize=14, font_family='Droid Sans Japanese')\n\nedge_width = [d['weight']*10 for (u,v,d) in G.edges(data=True)]\nnx.draw_networkx_edges(G, pos, alpha=0.2, edge_color='black', width=edge_width)\n\nplt.axis('off')\nplt.show()","metadata":{"_cell_guid":"55728464-e823-46c4-997b-b76258e23994","_uuid":"4e3ffc5ae7c2a49dfd2a0b151f0dab7da6c1bef5","trusted":true},"execution_count":null,"outputs":[]}]}