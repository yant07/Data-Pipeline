{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.13 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ca1a6c70ba0ef033524d16c288d1083a318dfe37e6b1f3c85cb25e894a9c8d20"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TextData \n",
    "## Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 英文"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "from html.parser import HTMLParser\n",
    "import string\n",
    "import re\n",
    "import itertools\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tweet=\"I enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is http://t.co/4ftYom0i It's awesome you'll luv it #HadFun #Enjoyed BFN GN\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I enjoyd the event which took place yesteday & I lovdddd itttt ! The link to the show is  It's awesome you'll luv it HadFun Enjoyed BFN GN\n"
     ]
    }
   ],
   "source": [
    "#去掉网址，#等非英文字符的内容\n",
    "def rm_nontext(text):\n",
    "    text_rmurl=html.unescape(text)\n",
    "    text = re.sub(r'https?:\\/\\/.\\S+', \"\", text_rmurl)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweet_rm1=rm_nontext(tweet)\n",
    "print(tweet_rm1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i enjoyed the event which took place yesterday & i loved itt ! the link to the show is it is awesome you will luv it had fun enjoyed bin gn\n"
     ]
    }
   ],
   "source": [
    " #缩写还原,用完整全称替换;分开单词,比如forthewin;统一转换为小写；俚语转换；词形还原；拼写检查\n",
    "def conv_text(text):\n",
    "    Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\n",
    "            \"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}\n",
    "    for key,value in Apos_dict.items():\n",
    "        if key in text:\n",
    "            text=text.replace(key,value)        \n",
    "###########################################################################            \n",
    "    text = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",text) if s])\n",
    "###########################################################################\n",
    "    text=text.lower()\n",
    "###########################################################################\n",
    "    file=open(\"./slang.txt\",\"r\")\n",
    "    slang=file.read()\n",
    "    slang=slang.split('\\n')\n",
    "    text_tokens=text.split()\n",
    "    slang_word=[]\n",
    "    meaning=[]\n",
    "\n",
    "    for line in slang:\n",
    "        temp=line.split(\"=\")\n",
    "        slang_word.append(temp[0])\n",
    "        meaning.append(temp[-1])\n",
    "\n",
    "    for i,word in enumerate(text_tokens):\n",
    "        if word in slang_word:\n",
    "            idx=slang_word.index(word)\n",
    "            text_tokens[i]=meaning[idx]\n",
    "            \n",
    "    text=\" \".join(text_tokens)\n",
    "#############################################################################\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    spell = Speller(lang='en')\n",
    "    #spell check\n",
    "    text=spell(text)\n",
    "    return text\n",
    "tweet_rm2=conv_text(tweet_rm1)\n",
    "print(tweet_rm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['enjoyed', 'event', 'took', 'place', 'yesterday', '&', 'loved', 'itt', '!', 'link', 'show', 'awesome', 'luv', 'fun', 'enjoyed', 'bin', 'gn']\n"
     ]
    }
   ],
   "source": [
    "#去停用词\n",
    "def rm_stopwords(text):\n",
    "    stopwords_eng = stopwords.words('english')\n",
    "    \n",
    "    text_tokens=text.split()\n",
    "    text_list=[]\n",
    "    #remove stopwords\n",
    "    for word in text_tokens:\n",
    "        if word not in stopwords_eng:\n",
    "            text_list.append(word)\n",
    "    return text_list\n",
    "textls=rm_stopwords(tweet_rm2)\n",
    "print(textls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['enjoyed', 'event', 'took', 'place', 'yesterday', 'loved', 'itt', 'link', 'show', 'awesome', 'luv', 'fun', 'enjoyed', 'bin', 'gn']\n"
     ]
    }
   ],
   "source": [
    "#去标点\n",
    "def rm_punc(text_list):\n",
    "    clean_text=[]\n",
    "    #remove punctuations\n",
    "    for word in text_list:\n",
    "        if word not in string.punctuation:\n",
    "            clean_text.append(word)\n",
    "    return clean_text\n",
    "text_fin=rm_punc(textls)\n",
    "print(text_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "# word_tokenize accepts\n",
    "# a string as an input, not a file.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file1 = open(\"text.txt\", encoding = 'utf-8')#需要指定encoding方式\n",
    " \n",
    "# Use this to read file content as a stream:\n",
    "line = file1.read()\n",
    "words = line.split()\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendFile = open('filteredtext.txt','a')\n",
    "        appendFile.write(\" \"+r)\n",
    "        appendFile.close()"
   ]
  },
  {
   "source": [
    "### 中文\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "原句子为:\n1,2,3...how ara you 开始吧， 加油！乳腺癌是乳腺上皮细胞在多种致癌因子的作用下，发生增殖失控的现象。疾病早期常表现为乳房肿块、乳头溢液、腋窝淋巴结肿大等症状，晚期可因癌细胞发生远处转移，出现多器官病变，直接威胁患者的生命。\n\n处理后的句子为:\n开始吧加油乳腺癌是乳腺上皮细胞在多种致癌因子的作用下发生增殖失控的现象疾病早期常表现为乳房肿块乳头溢液腋窝淋巴结肿大等症状晚期可因癌细胞发生远处转移出现多器官病变直接威胁患者的生命\n"
     ]
    }
   ],
   "source": [
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':  # 判断一个uchar是否是汉字\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    " \n",
    "def allcontents(contents):\n",
    "    content = ''\n",
    "    for i in contents:\n",
    "        if is_chinese(i):\n",
    "            content = content+i\n",
    "    print('\\n处理后的句子为:\\n'+content)\n",
    " \n",
    "centents = '1,2,3...how ara you 开始吧， 加油！乳腺癌是乳腺上皮细胞在多种致癌因子的作用下，发生增殖失控的现象。疾病早期常表现为乳房肿块、乳头溢液、腋窝淋巴结肿大等症状，晚期可因癌细胞发生远处转移，出现多器官病变，直接威胁患者的生命。'\n",
    "print('原句子为:\\n'+centents)\n",
    "allcontents(centents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "原句子为:\n开始吧加油,duo rui mi,冲冲冲*_*乳腺癌是乳腺上皮细胞在多种致癌因子的作用下，发生增殖失控的现象。疾病早期常表现为乳房肿块、乳头溢液、腋窝淋巴结肿大等症状，晚期可因癌细胞发生远处转移，出现多器官病变，直接威胁患者的生命。\n\n处理后的句子为:\n开始吧加油,duoruimi,冲冲冲乳腺癌是乳腺上皮细胞在多种致癌因子的作用下发生增殖失控的现象疾病早期常表现为乳房肿块乳头溢液腋窝淋巴结肿大等症状晚期可因癌细胞发生远处转移出现多器官病变直接威胁患者的生命\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence='开始吧加油,duo rui mi,冲冲冲*_*乳腺癌是乳腺上皮细胞在多种致癌因子的作用下，发生增殖失控的现象。疾病早期常表现为乳房肿块、乳头溢液、腋窝淋巴结肿大等症状，晚期可因癌细胞发生远处转移，出现多器官病变，直接威胁患者的生命。'\n",
    "print('原句子为:\\n'+sentence)\n",
    " \n",
    "def clear_character(sentence):    \n",
    "    pattern = re.compile(\"[^\\u4e00-\\u9fa5^,^.^!^a-z^A-Z^0-9]\")  #只保留中英文、数字和符号，去掉其他东西\n",
    "    #若只保留中英文和数字，则替换为[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\n",
    "    line=re.sub(pattern,'',sentence)  #把文本中匹配到的字符替换成空字符\n",
    "    new_sentence=''.join(line.split())    #去除空白\n",
    "    print('\\n处理后的句子为:\\n'+new_sentence) \n",
    " \n",
    "clear_character(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['开'], ['始'], ['吧'], ['加'], ['油'], [','], ['d'], ['u'], ['o'], [' '], ['r'], ['u'], ['i'], [' '], ['m'], ['i'], [','], ['冲'], ['冲'], ['冲'], ['*'], ['_'], ['*'], ['乳'], ['腺'], ['癌'], ['是'], ['乳'], ['腺'], ['上'], ['皮'], ['细'], ['胞'], ['在'], ['多'], ['种'], ['致'], ['癌'], ['因'], ['子'], ['的'], ['作'], ['用'], ['下'], ['，'], ['发'], ['生'], ['增'], ['殖'], ['失'], ['控'], ['的'], ['现'], ['象'], ['。'], ['疾'], ['病'], ['早'], ['期'], ['常'], ['表'], ['现'], ['为'], ['乳'], ['房'], ['肿'], ['块'], ['、'], ['乳'], ['头'], ['溢'], ['液'], ['、'], ['腋'], ['窝'], ['淋'], ['巴'], ['结'], ['肿'], ['大'], ['等'], ['症'], ['状'], ['，'], ['晚'], ['期'], ['可'], ['因'], ['癌'], ['细'], ['胞'], ['发'], ['生'], ['远'], ['处'], ['转'], ['移'], ['，'], ['出'], ['现'], ['多'], ['器'], ['官'], ['病'], ['变'], ['，'], ['直'], ['接'], ['威'], ['胁'], ['患'], ['者'], ['的'], ['生'], ['命'], ['。']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 对文本进行jieba分词\n",
    "\n",
    "def fenci(datas):\n",
    "    cut_words = map(lambda s: list(jieba.cut(s)), datas)\n",
    "    return list(cut_words)\n",
    "fenci_list=fenci(sentence)\n",
    "print(fenci(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['开'], ['始'], ['吧'], ['加'], ['油'], [','], ['d'], ['u'], ['o'], [' '], ['r'], ['u'], ['i'], [' '], ['m'], ['i'], [','], ['冲'], ['冲'], ['冲'], ['*'], ['_'], ['*'], ['乳'], ['腺'], ['癌'], ['是'], ['乳'], ['腺'], ['上'], ['皮'], ['细'], ['胞'], ['在'], ['多'], ['种'], ['致'], ['癌'], ['因'], ['子'], [], ['作'], ['用'], ['下'], ['，'], ['发'], ['生'], ['增'], ['殖'], ['失'], ['控'], [], ['现'], ['象'], ['。'], ['疾'], ['病'], ['早'], ['期'], ['常'], ['表'], ['现'], ['为'], ['乳'], ['房'], ['肿'], ['块'], ['、'], ['乳'], ['头'], ['溢'], ['液'], ['、'], ['腋'], ['窝'], ['淋'], ['巴'], ['结'], ['肿'], ['大'], ['等'], ['症'], ['状'], ['，'], ['晚'], ['期'], ['可'], ['因'], ['癌'], ['细'], ['胞'], ['发'], ['生'], ['远'], ['处'], ['转'], ['移'], ['，'], ['出'], ['现'], ['多'], ['器'], ['官'], ['病'], ['变'], ['，'], ['直'], ['接'], ['威'], ['胁'], ['患'], ['者'], [], ['生'], ['命'], ['。']]\n"
     ]
    }
   ],
   "source": [
    "# 停用词表\n",
    "stopwords = ['的','呀','这','那','就','的话','如果']\n",
    "\n",
    "# 去掉文本中的停用词\n",
    "def drop_stopwords(contents, stopwords):\n",
    "    contents_clean = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean\n",
    "\n",
    "print(drop_stopwords(fenci_list,stopwords))\n"
   ]
  }
 ]
}